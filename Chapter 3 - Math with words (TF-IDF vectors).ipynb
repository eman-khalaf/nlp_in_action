{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fe7236d",
   "metadata": {},
   "source": [
    "### This chapter covers\n",
    " Counting words and term frequencies to analyze meaning\n",
    "\n",
    " Predicting word occurrence probabilities with Zipf’s Law\n",
    "\n",
    " Vector representation of words and how to start using them\n",
    "\n",
    " Finding relevant documents from a corpus using inverse document frequencies\n",
    "\n",
    " Estimating the similarity of pairs of documents with cosine similarity and Okapi BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40b5d00",
   "metadata": {},
   "source": [
    "Detecting words is useful for simple tasks, like getting statistics about word usage or doing keyword search. Then we can use that “importance” value to find relevant documents in a corpus based on keyword importance within each document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffd14bb",
   "metadata": {},
   "source": [
    "##### In this chapter, we look at three increasingly powerful ways to represent words and their importance in a document:\n",
    " Bags of words—Vectors of word counts or frequencies\n",
    "\n",
    " Bags of n-grams—Counts of word pairs (bigrams), triplets (trigrams), and so on\n",
    "\n",
    " TF-IDF vectors—Word scores that better represent their importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb78bc1",
   "metadata": {},
   "source": [
    "Note : A document that refers to “wings” and “rudder” frequently may be more relevant to a problem involving jet airplanes or air travel, than say a document that refers frequently to “cats” and “gravity.” Or if we have classified some words as expressing positive emotions—words like “good,” “best,” “joy,” and “fantastic”—the more a document that contains those words is likely to have positive “sentiment.”\n",
    "##### Let’s look at an example where counting occurrences of words is useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8b3afc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3e7e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6f59171",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"The faster Harry got to the store, the faster Harry, the faster, would get home.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba85dd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10d0082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(sentence.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fcf85c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'faster',\n",
       " 'harry',\n",
       " 'got',\n",
       " 'to',\n",
       " 'the',\n",
       " 'store',\n",
       " ',',\n",
       " 'the',\n",
       " 'faster',\n",
       " 'harry',\n",
       " ',',\n",
       " 'the',\n",
       " 'faster',\n",
       " ',',\n",
       " 'would',\n",
       " 'get',\n",
       " 'home',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0705d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e5b1ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ccb29d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 4,\n",
       "         'faster': 3,\n",
       "         'harry': 2,\n",
       "         'got': 1,\n",
       "         'to': 1,\n",
       "         'store': 1,\n",
       "         ',': 3,\n",
       "         'would': 1,\n",
       "         'get': 1,\n",
       "         'home': 1,\n",
       "         '.': 1})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5bed48",
   "metadata": {},
   "source": [
    "###### Counter object has a handy method, most_common, for just this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "186a58e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 4), ('faster', 3), (',', 3), ('harry', 2)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.most_common(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f69a53",
   "metadata": {},
   "source": [
    "Specifically, the number of times a word occurs in a given document is called the term frequency, commonly abbreviated TF. In some examples we may see the count of word occurrences normalized (divided) by the number of terms in the document. However, normalized frequency is really a probability, so it should probably not be called frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da35ef7e",
   "metadata": {},
   "source": [
    "Let’s calculate the term frequency of “harry” from the Counter object (bag_of_words) you defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "544c13d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "times_harry_appears = bag_of_words['harry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48e1ff59",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_words = len(bag_of_words) #The number of unique tokens from your original source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "027e825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = times_harry_appears / num_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d3c9536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1818"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(tf, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fe9fbc",
   "metadata": {},
   "source": [
    "Take these first few paragraphs from the Wikipedia article on kites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "458fe597",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kite_text1 = \"\"\"A kite is traditionally a tethered heavier-than-air craft with wing surfaces that react\n",
    "against the air to create lift and drag. A kite consists of wings, tethers, and anchors.\n",
    "Kites often have a bridle to guide the face of the kite at the correct angle so the wind\n",
    "can lift it. A kite’s wing also may be so designed so a bridle is not needed; when\n",
    "kiting a sailplane for launch, the tether meets the wing at a single point. A kite may\n",
    "have fixed or moving anchors. Untraditionally in technical kiting, a kite consists of\n",
    "tether-set-coupled wing sets; even in technical kiting, though, a wing in the system is\n",
    "still often called the kite.\n",
    "The lift that sustains the kite in flight is generated when air flows around the kite’s\n",
    "surface, producing low pressure above and high pressure below the wings. The\n",
    "interaction with the wind also generates horizontal drag along the direction of the\n",
    "wind. The resultant force vector from the lift and drag force components is opposed\n",
    "by the tension of one or more of the lines or tethers to which the kite is attached. The\n",
    "anchor point of the kite line may be static or moving (such as the towing of a kite by\n",
    "a running person, boat, free-falling anchors as in paragliders and fugitive parakites\n",
    "or vehicle).\n",
    "The same principles of fluid flow apply in liquids and kites are also used under water.\n",
    "A hybrid tethered craft comprising both a lighter-than-air balloon as well as a kite\n",
    "lifting surface is called a kytoon.\n",
    "Kites have a long and varied history and many different types are flown\n",
    "individually and at festivals worldwide. Kites may be flown for recreation, art or\n",
    "other practical uses. Sport kites can be flown in aerial ballet, sometimes as part of a\n",
    "competition. Power kites are multi-line steerable kites designed to generate large forces\n",
    "which can be used to power activities such as kite surfing, kite landboarding, kite\n",
    "fishing, kite buggying and a new trend snow kiting. Even Man-lifting kites have\n",
    "been made.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f3fce93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3ca1cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89d46baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eff03346",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nlpia'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnlpia\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m kite_text\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nlpia'"
     ]
    }
   ],
   "source": [
    "from nlpia.data.loaders import kite_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54dcc48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(kite_text1.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9dc36ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab1cf7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'a': 20,\n",
       "         'kite': 14,\n",
       "         'is': 7,\n",
       "         'traditionally': 1,\n",
       "         'tethered': 2,\n",
       "         'heavier-than-air': 1,\n",
       "         'craft': 2,\n",
       "         'with': 2,\n",
       "         'wing': 5,\n",
       "         'surfaces': 1,\n",
       "         'that': 2,\n",
       "         'react': 1,\n",
       "         'against': 1,\n",
       "         'the': 26,\n",
       "         'air': 2,\n",
       "         'to': 5,\n",
       "         'create': 1,\n",
       "         'lift': 4,\n",
       "         'and': 10,\n",
       "         'drag.': 1,\n",
       "         'consists': 2,\n",
       "         'of': 10,\n",
       "         'wings': 1,\n",
       "         ',': 14,\n",
       "         'tethers': 2,\n",
       "         'anchors.': 2,\n",
       "         'kites': 8,\n",
       "         'often': 2,\n",
       "         'have': 4,\n",
       "         'bridle': 2,\n",
       "         'guide': 1,\n",
       "         'face': 1,\n",
       "         'at': 3,\n",
       "         'correct': 1,\n",
       "         'angle': 1,\n",
       "         'so': 3,\n",
       "         'wind': 2,\n",
       "         'can': 3,\n",
       "         'it.': 1,\n",
       "         'kite’s': 2,\n",
       "         'also': 3,\n",
       "         'may': 4,\n",
       "         'be': 5,\n",
       "         'designed': 2,\n",
       "         'not': 1,\n",
       "         'needed': 1,\n",
       "         ';': 2,\n",
       "         'when': 2,\n",
       "         'kiting': 3,\n",
       "         'sailplane': 1,\n",
       "         'for': 2,\n",
       "         'launch': 1,\n",
       "         'tether': 1,\n",
       "         'meets': 1,\n",
       "         'single': 1,\n",
       "         'point.': 1,\n",
       "         'fixed': 1,\n",
       "         'or': 6,\n",
       "         'moving': 2,\n",
       "         'untraditionally': 1,\n",
       "         'in': 7,\n",
       "         'technical': 2,\n",
       "         'tether-set-coupled': 1,\n",
       "         'sets': 1,\n",
       "         'even': 2,\n",
       "         'though': 1,\n",
       "         'system': 1,\n",
       "         'still': 1,\n",
       "         'called': 2,\n",
       "         'kite.': 1,\n",
       "         'sustains': 1,\n",
       "         'flight': 1,\n",
       "         'generated': 1,\n",
       "         'flows': 1,\n",
       "         'around': 1,\n",
       "         'surface': 2,\n",
       "         'producing': 1,\n",
       "         'low': 1,\n",
       "         'pressure': 2,\n",
       "         'above': 1,\n",
       "         'high': 1,\n",
       "         'below': 1,\n",
       "         'wings.': 1,\n",
       "         'interaction': 1,\n",
       "         'generates': 1,\n",
       "         'horizontal': 1,\n",
       "         'drag': 2,\n",
       "         'along': 1,\n",
       "         'direction': 1,\n",
       "         'wind.': 1,\n",
       "         'resultant': 1,\n",
       "         'force': 2,\n",
       "         'vector': 1,\n",
       "         'from': 1,\n",
       "         'components': 1,\n",
       "         'opposed': 1,\n",
       "         'by': 2,\n",
       "         'tension': 1,\n",
       "         'one': 1,\n",
       "         'more': 1,\n",
       "         'lines': 1,\n",
       "         'which': 2,\n",
       "         'attached.': 1,\n",
       "         'anchor': 1,\n",
       "         'point': 1,\n",
       "         'line': 1,\n",
       "         'static': 1,\n",
       "         '(': 1,\n",
       "         'such': 2,\n",
       "         'as': 6,\n",
       "         'towing': 1,\n",
       "         'running': 1,\n",
       "         'person': 1,\n",
       "         'boat': 1,\n",
       "         'free-falling': 1,\n",
       "         'anchors': 1,\n",
       "         'paragliders': 1,\n",
       "         'fugitive': 1,\n",
       "         'parakites': 1,\n",
       "         'vehicle': 1,\n",
       "         ')': 1,\n",
       "         '.': 2,\n",
       "         'same': 1,\n",
       "         'principles': 1,\n",
       "         'fluid': 1,\n",
       "         'flow': 1,\n",
       "         'apply': 1,\n",
       "         'liquids': 1,\n",
       "         'are': 3,\n",
       "         'used': 2,\n",
       "         'under': 1,\n",
       "         'water.': 1,\n",
       "         'hybrid': 1,\n",
       "         'comprising': 1,\n",
       "         'both': 1,\n",
       "         'lighter-than-air': 1,\n",
       "         'balloon': 1,\n",
       "         'well': 1,\n",
       "         'lifting': 1,\n",
       "         'kytoon.': 1,\n",
       "         'long': 1,\n",
       "         'varied': 1,\n",
       "         'history': 1,\n",
       "         'many': 1,\n",
       "         'different': 1,\n",
       "         'types': 1,\n",
       "         'flown': 3,\n",
       "         'individually': 1,\n",
       "         'festivals': 1,\n",
       "         'worldwide.': 1,\n",
       "         'recreation': 1,\n",
       "         'art': 1,\n",
       "         'other': 1,\n",
       "         'practical': 1,\n",
       "         'uses.': 1,\n",
       "         'sport': 1,\n",
       "         'aerial': 1,\n",
       "         'ballet': 1,\n",
       "         'sometimes': 1,\n",
       "         'part': 1,\n",
       "         'competition.': 1,\n",
       "         'power': 2,\n",
       "         'multi-line': 1,\n",
       "         'steerable': 1,\n",
       "         'generate': 1,\n",
       "         'large': 1,\n",
       "         'forces': 1,\n",
       "         'activities': 1,\n",
       "         'surfing': 1,\n",
       "         'landboarding': 1,\n",
       "         'fishing': 1,\n",
       "         'buggying': 1,\n",
       "         'new': 1,\n",
       "         'trend': 1,\n",
       "         'snow': 1,\n",
       "         'kiting.': 1,\n",
       "         'man-lifting': 1,\n",
       "         'been': 1,\n",
       "         'made': 1})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8937c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15f12be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e47ca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8ee9431",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [x for x in tokens if x not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "800d9b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "kite_counts = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ccf3bacb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'kite': 14,\n",
       "         'traditionally': 1,\n",
       "         'tethered': 2,\n",
       "         'heavier-than-air': 1,\n",
       "         'craft': 2,\n",
       "         'wing': 5,\n",
       "         'surfaces': 1,\n",
       "         'react': 1,\n",
       "         'air': 2,\n",
       "         'create': 1,\n",
       "         'lift': 4,\n",
       "         'drag.': 1,\n",
       "         'consists': 2,\n",
       "         'wings': 1,\n",
       "         ',': 14,\n",
       "         'tethers': 2,\n",
       "         'anchors.': 2,\n",
       "         'kites': 8,\n",
       "         'often': 2,\n",
       "         'bridle': 2,\n",
       "         'guide': 1,\n",
       "         'face': 1,\n",
       "         'correct': 1,\n",
       "         'angle': 1,\n",
       "         'wind': 2,\n",
       "         'it.': 1,\n",
       "         'kite’s': 2,\n",
       "         'also': 3,\n",
       "         'may': 4,\n",
       "         'designed': 2,\n",
       "         'needed': 1,\n",
       "         ';': 2,\n",
       "         'kiting': 3,\n",
       "         'sailplane': 1,\n",
       "         'launch': 1,\n",
       "         'tether': 1,\n",
       "         'meets': 1,\n",
       "         'single': 1,\n",
       "         'point.': 1,\n",
       "         'fixed': 1,\n",
       "         'moving': 2,\n",
       "         'untraditionally': 1,\n",
       "         'technical': 2,\n",
       "         'tether-set-coupled': 1,\n",
       "         'sets': 1,\n",
       "         'even': 2,\n",
       "         'though': 1,\n",
       "         'system': 1,\n",
       "         'still': 1,\n",
       "         'called': 2,\n",
       "         'kite.': 1,\n",
       "         'sustains': 1,\n",
       "         'flight': 1,\n",
       "         'generated': 1,\n",
       "         'flows': 1,\n",
       "         'around': 1,\n",
       "         'surface': 2,\n",
       "         'producing': 1,\n",
       "         'low': 1,\n",
       "         'pressure': 2,\n",
       "         'high': 1,\n",
       "         'wings.': 1,\n",
       "         'interaction': 1,\n",
       "         'generates': 1,\n",
       "         'horizontal': 1,\n",
       "         'drag': 2,\n",
       "         'along': 1,\n",
       "         'direction': 1,\n",
       "         'wind.': 1,\n",
       "         'resultant': 1,\n",
       "         'force': 2,\n",
       "         'vector': 1,\n",
       "         'components': 1,\n",
       "         'opposed': 1,\n",
       "         'tension': 1,\n",
       "         'one': 1,\n",
       "         'lines': 1,\n",
       "         'attached.': 1,\n",
       "         'anchor': 1,\n",
       "         'point': 1,\n",
       "         'line': 1,\n",
       "         'static': 1,\n",
       "         '(': 1,\n",
       "         'towing': 1,\n",
       "         'running': 1,\n",
       "         'person': 1,\n",
       "         'boat': 1,\n",
       "         'free-falling': 1,\n",
       "         'anchors': 1,\n",
       "         'paragliders': 1,\n",
       "         'fugitive': 1,\n",
       "         'parakites': 1,\n",
       "         'vehicle': 1,\n",
       "         ')': 1,\n",
       "         '.': 2,\n",
       "         'principles': 1,\n",
       "         'fluid': 1,\n",
       "         'flow': 1,\n",
       "         'apply': 1,\n",
       "         'liquids': 1,\n",
       "         'used': 2,\n",
       "         'water.': 1,\n",
       "         'hybrid': 1,\n",
       "         'comprising': 1,\n",
       "         'lighter-than-air': 1,\n",
       "         'balloon': 1,\n",
       "         'well': 1,\n",
       "         'lifting': 1,\n",
       "         'kytoon.': 1,\n",
       "         'long': 1,\n",
       "         'varied': 1,\n",
       "         'history': 1,\n",
       "         'many': 1,\n",
       "         'different': 1,\n",
       "         'types': 1,\n",
       "         'flown': 3,\n",
       "         'individually': 1,\n",
       "         'festivals': 1,\n",
       "         'worldwide.': 1,\n",
       "         'recreation': 1,\n",
       "         'art': 1,\n",
       "         'practical': 1,\n",
       "         'uses.': 1,\n",
       "         'sport': 1,\n",
       "         'aerial': 1,\n",
       "         'ballet': 1,\n",
       "         'sometimes': 1,\n",
       "         'part': 1,\n",
       "         'competition.': 1,\n",
       "         'power': 2,\n",
       "         'multi-line': 1,\n",
       "         'steerable': 1,\n",
       "         'generate': 1,\n",
       "         'large': 1,\n",
       "         'forces': 1,\n",
       "         'activities': 1,\n",
       "         'surfing': 1,\n",
       "         'landboarding': 1,\n",
       "         'fishing': 1,\n",
       "         'buggying': 1,\n",
       "         'new': 1,\n",
       "         'trend': 1,\n",
       "         'snow': 1,\n",
       "         'kiting.': 1,\n",
       "         'man-lifting': 1,\n",
       "         'made': 1})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kite_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13a6799",
   "metadata": {},
   "source": [
    "#### Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20fd6082",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vector = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7a47652",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_length = len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "702036ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in kite_counts.most_common(): document_vector.append(value / doc_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b54d14a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.06422018348623854,\n",
       " 0.06422018348623854,\n",
       " 0.03669724770642202,\n",
       " 0.022935779816513763,\n",
       " 0.01834862385321101,\n",
       " 0.01834862385321101,\n",
       " 0.013761467889908258,\n",
       " 0.013761467889908258,\n",
       " 0.013761467889908258,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.009174311926605505,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525,\n",
       " 0.0045871559633027525]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2f1826",
   "metadata": {},
   "source": [
    "we can grab a couple more documents and make vectors for each of them as well. But the values within each vector need to be relative to something consistent across all the vectors. If we’re going to do math on them, they need to represent\n",
    "a position in a common space, relative to something consistent. \n",
    "###### The first step in this process is to normalize the counts by calculating normalized term frequency instead of raw count in the document (as you did in the last section); the second step is to make all the vectors of standard length or dimension.\n",
    "\n",
    "You’ll find every unique word in each document and then find every unique ord in the union of those two sets. This collections of words in your vocabulary is ften called a lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e5ec21",
   "metadata": {},
   "source": [
    "Let’s check in on Harry. You had one “document” already— let’s round out the corpus with a couple more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bfed59",
   "metadata": {},
   "source": [
    "docs = [\"The faster Harry got to the store, the faster and faster Harry  would get home.\"] # we use append to add to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb93da31",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.append(\"Harry is hairy and faster than Jill.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5bdd7149",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.append(\"Jill is not as hairy as Harry.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f87218f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2f9effd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    doc_tokens += [sorted(tokenizer.tokenize(doc.lower()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f5ff87a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f24acf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_doc_tokens = sum(doc_tokens, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee8a77ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_doc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8f336ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = sorted(set(all_doc_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2f41e410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ed0448e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " '.',\n",
       " 'and',\n",
       " 'as',\n",
       " 'faster',\n",
       " 'get',\n",
       " 'got',\n",
       " 'hairy',\n",
       " 'harry',\n",
       " 'home',\n",
       " 'is',\n",
       " 'jill',\n",
       " 'not',\n",
       " 'store',\n",
       " 'than',\n",
       " 'the',\n",
       " 'to',\n",
       " 'would']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eaac2c",
   "metadata": {},
   "source": [
    "Each of your three document vectors will need to have 18 values, even if the document for that vector doesn’t contain all 18 words in your lexicon. Each token is assigned a “slot” in your vectors corresponding to its position in your lexicon. Some of those token counts in the vector will be zeros, which is what you want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "614dcec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5dc1d481",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_vector = OrderedDict((token, 0) for token in lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fda4d02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(',', 0),\n",
       "             ('.', 0),\n",
       "             ('and', 0),\n",
       "             ('as', 0),\n",
       "             ('faster', 0),\n",
       "             ('get', 0),\n",
       "             ('got', 0),\n",
       "             ('hairy', 0),\n",
       "             ('harry', 0),\n",
       "             ('home', 0),\n",
       "             ('is', 0),\n",
       "             ('jill', 0),\n",
       "             ('not', 0),\n",
       "             ('store', 0),\n",
       "             ('than', 0),\n",
       "             ('the', 0),\n",
       "             ('to', 0),\n",
       "             ('would', 0)])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "62fcb1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we’ll make copies of that base vector, update the values of the vector for each document, and store them in an array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "640b45d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a090c2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vectors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f621a0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    vec = copy.copy(zero_vector)\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "    for key, value in token_counts.items():\n",
    "        vec[key] = value / len(lexicon)\n",
    "    doc_vectors.append(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb4716a",
   "metadata": {},
   "source": [
    "copy.copy() creates an independent copy, a separate instance of your zero vector, rather than reusing a reference (pointer) to\n",
    "the original object’s memory location. Otherwise you’d just be overwriting the same zero_vector with new values in each loop, and you wouldn’t have a fresh zero on each pass of the loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48ee8fe",
   "metadata": {},
   "source": [
    "###### sometimes we call this dimensionality capital letter “K.” This number of distinct words is also the vocabulary size of your corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04090151",
   "metadata": {},
   "source": [
    "Cosine similarity is merely the cosine of the angle between two vectors (theta), shown in figure 3.3, which can be calculated from the Euclidian dot product using Cosine similarity is efficient to calculate because the dot product doesn’t require evaluation of any trigonometric functions. In addition, cosine similarity has a convenient range for most machine learning problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365cb11b",
   "metadata": {},
   "source": [
    "In Python this would be\n",
    "\n",
    "a.dot(b) == np.linalg.norm(a) * np.linalg.norm(b) / np.cos(theta)\n",
    "\n",
    "Solving this relationship for cos(theta), you can derive the cosine similarity using\n",
    "###### Or you can do it in pure Python without numpy, as in the following listing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96408ff4",
   "metadata": {},
   "source": [
    "###### Compute cosine similarity in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c208c455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9edcd079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(vec1, vec2):\n",
    "    \"\"\" Let's convert our dictionaries to lists for easier matching.\"\"\"\n",
    "    vec1 = [val for val in vec1.values()]\n",
    "    vec2 = [val for val in vec2.values()]\n",
    "    dot_prod = 0\n",
    "    for i, v in enumerate(vec1):\n",
    "         dot_prod += v * vec2[i]\n",
    "    mag_1 = math.sqrt(sum([x**2 for x in vec1]))\n",
    "    mag_2 = math.sqrt(sum([x**2 for x in vec2]))\n",
    "    return dot_prod / (mag_1 * mag_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d3bd7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6662f86f",
   "metadata": {},
   "source": [
    "#### Zipf’s Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6aebf4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\EMZ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')  #The Brown corpus is about 3MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4d9c1b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c3f469ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words()[:10]  # words() is a built-in method of the NTLK corpus object that returns the tokenized corpus as a sequence of strs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "90ed7fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('Fulton', 'NP-TL'),\n",
       " ('County', 'NN-TL'),\n",
       " ('Grand', 'JJ-TL'),\n",
       " ('Jury', 'NN-TL')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.tagged_words()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0736c207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "17647a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0d8c1c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "puncs = set((',', '.', '--', '-', '!', '?', ':', ';', '``', \"''\", '(', ')', '[', ']'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "41bd90c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = (x.lower() for x in brown.words() if x not in puncs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d54d0a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = Counter(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a85b365f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 69971),\n",
       " ('of', 36412),\n",
       " ('and', 28853),\n",
       " ('to', 26158),\n",
       " ('a', 23195),\n",
       " ('in', 21337),\n",
       " ('that', 10594),\n",
       " ('is', 10109),\n",
       " ('was', 9815),\n",
       " ('he', 9548),\n",
       " ('for', 9489),\n",
       " ('it', 8760),\n",
       " ('with', 7289),\n",
       " ('as', 7253),\n",
       " ('his', 6996),\n",
       " ('on', 6741),\n",
       " ('be', 6377),\n",
       " ('at', 5372),\n",
       " ('by', 5306),\n",
       " ('i', 5164)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e998cbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check https://github.com/totalgood/nlpia/blob/master/src/nlpia/book/examples/ch03_zipf.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca9a394",
   "metadata": {},
   "source": [
    "##### Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce7dd42",
   "metadata": {},
   "source": [
    "Let’s return to the Kite example from Wikipedia and grab another section (the History section); say it’s the second document in the Kite corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98305182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a86b7d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kite_history = \"\"\"Kites were invented in China, where materials ideal for kite building were readily available: \n",
    "silk fabric for sail material; fine, high-tensile-strength silk for flying line; and resilient bamboo for a strong, \n",
    "lightweight framework.\n",
    "The kite has been claimed as the invention of the 5th-century BC Chinese philosophers Mozi (also Mo Di) and Lu Ban \n",
    "(also Gongshu Ban). By 549 AD paper kites were certainly being flown, as it was recorded that in that year a paper kite\n",
    "was used as a message for a rescue mission. Ancient and medieval Chinese sources describe kites being used for measuring \n",
    "distances, testing the wind, lifting men, signaling, and communication for military operations. The earliest known Chinese \n",
    "kites were flat (not bowed) and often rectangular. Later, tailless kites incorporated a stabilizing bowline. Kites were \n",
    "decorated with mythological motifs and legendary figures; some were fitted with strings and whistles to make musical sounds \n",
    "while flying. From China, kites were introduced to Cambodia, Thailand, India, Japan, Korea and the western world.\n",
    "After its introduction into India, the kite further evolved into the fighter kite, known as the patang in India, where\n",
    "thousands are flown every year on festivals such as Makar Sankranti. Kites were known throughout Polynesia, as far as \n",
    "New Zealand, with the assumption being that the knowledge diffused from China along with the people. Anthropomorphic kites \n",
    "made from cloth and wood were used in religious ceremonies to send prayers to the gods. Polynesian kite traditions are used\n",
    "by anthropologists get an idea of early “primitive” Asian traditions that are believed to have at one time existed in Asia.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "024f26d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kite_intro = kite_text1.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f7b11834",
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_tokens = tokenizer.tokenize(kite_intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1d769d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "kite_history = kite_history.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0e80882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_tokens = tokenizer.tokenize(kite_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3f6c1633",
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_total = len(intro_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "243312d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "361"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intro_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "88e9ca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_total = len(history_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1eec4b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "295"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556ad762",
   "metadata": {},
   "source": [
    "Now with a couple tokenized kite documents in hand, let’s look at the term frequency of “kite” in each document. we ’ll store the TFs you find in two dictionaries, one for each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "39c51740",
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_tf = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "91366856",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_tf = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "84a0684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_counts = Counter(intro_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "10f97a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_tf['kite'] = intro_counts['kite'] / intro_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ef269f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_counts = Counter(history_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "73d525e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_tf['kite'] = history_counts['kite'] / history_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "62b121e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Term Frequency of \"kite\" in intro is: 0.0388'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Term Frequency of \"kite\" in intro is: {:.4f}'.format(intro_tf['kite']) ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ea1ede6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Term Frequency of \"kite\" in history is: 0.0203'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Term Frequency of \"kite\" in history is: {:.4f}'.format(history_tf['kite'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0b2a14c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_tf['and'] = intro_counts['and'] / intro_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "53684c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_tf['and'] = history_counts['and'] / history_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "43191b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency of \"and\" in intro is: 0.0277\n"
     ]
    }
   ],
   "source": [
    "print('Term Frequency of \"and\" in intro is: {:.4f}'.format(intro_tf['and']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fd37a908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency of \"and\" in history is: 0.0305\n"
     ]
    }
   ],
   "source": [
    "print('Term Frequency of \"and\" in history is: {:.4f}'.format(history_tf['and']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f5e742",
   "metadata": {},
   "source": [
    "A term’s IDF is merely the ratio of the total number of documents to the number of documents the term appears in. In the case of “and” and “kite” in this current example, the answer is the same for both:\n",
    "\n",
    " 2 total documents / 2 documents contain “and” = 2/2 = 1\n",
    "\n",
    " 2 total documents / 2 documents contain “kite” = 2/2 = 1\n",
    "\n",
    " Not very interesting. So let’s look at another word “China.”\n",
    "\n",
    " 2 total documents / 1 document contains “China” = 2/1 = 2\n",
    "\n",
    "##### Okay, that’s something different. Let’s use this “rarity” measure to weight the term frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "69e6dfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs_containing_and = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "48a882f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in [intro_tokens, history_tokens]:\n",
    "    if 'and' in doc:\n",
    "        num_docs_containing_and += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0fb4bf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs_containing_kite = 0\n",
    "for doc in [intro_tokens, history_tokens]:\n",
    "    if 'kite' in doc:\n",
    "        num_docs_containing_kite += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b2d53233",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the TF of “China” in the two documents\n",
    "intro_tf['china'] = intro_counts['china'] / intro_total \n",
    "history_tf['china'] = history_counts['china'] / history_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a64c4e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs_containing_china = 0\n",
    "for doc in [intro_tokens, history_tokens]:\n",
    "    if 'china' in doc:\n",
    "        num_docs_containing_china += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b31712",
   "metadata": {},
   "source": [
    "the IDF for all three. You’ll store the IDFs in dictionaries per document like you did with TF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "02ac52b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = 2\n",
    "intro_idf = {}\n",
    "history_idf = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9aed33f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_idf['and'] = num_docs / num_docs_containing_and\n",
    "history_idf['and'] = num_docs / num_docs_containing_and\n",
    "\n",
    "intro_idf['kite'] = num_docs / num_docs_containing_kite\n",
    "history_idf['kite'] = num_docs / num_docs_containing_kite\n",
    "\n",
    "intro_idf['china'] = num_docs / num_docs_containing_china\n",
    "history_idf['china'] = num_docs / num_docs_containing_china"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb285a55",
   "metadata": {},
   "source": [
    "###### And then for the intro document we find:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4643e7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_tfidf = {}\n",
    "\n",
    "intro_tfidf['and'] = intro_tf['and'] * intro_idf['and']\n",
    "intro_tfidf['kite'] = intro_tf['kite'] * intro_idf['kite']\n",
    "intro_tfidf['china'] = intro_tf['china'] * intro_idf['china']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80541e5",
   "metadata": {},
   "source": [
    "###### And then for the history document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "28be8610",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_tfidf = {}\n",
    "\n",
    "history_tfidf['and'] = history_tf['and'] * history_idf['and']\n",
    "history_tfidf['kite'] = history_tf['kite'] * history_idf['kite']\n",
    "history_tfidf['china'] = history_tf['china'] * history_idf['china']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2f5dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c822e4",
   "metadata": {},
   "source": [
    "##### Return of Zipf\n",
    "\n",
    "Let’s say, though, you have a corpus of 1 million documents (maybe you’re baby-Google), someone searches for the word “cat,” and in your 1 million documents you have exactly 1 document that contains the word “cat.” The raw\n",
    "IDF of this is\n",
    "1,000,000 / 1 = 1,000,000\n",
    "Let’s imagine you have 10 documents with the word “dog” in them. Your IDF for “dog” is\n",
    "1,000,000 / 10 = 100,000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dccae5b",
   "metadata": {},
   "source": [
    "That’s a big difference. So Zipf’s Law suggests that you scale all your word frequencies (and document frequencies)\n",
    "with the log() function, the inverse of exp(). This ensures that words such as “cat” and “dog,” which have similar counts, aren’t exponentially different in frequency. And this distribution of word frequencies will ensure that your TF-IDF scores are more uniformly distributed. So you should redefine IDF to be the log of the original probability of that word occurring in one of your documents. You’ll want to take the log of the term frequency as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50abf136",
   "metadata": {},
   "source": [
    "And then finally, for a given term, t, in a given document, d, in a corpus, D, you get:\n",
    "    \n",
    "tf(t, d) = count(t)/count(d)\n",
    "idf(t, D) = log (number of documents/number of documents containing t)\n",
    "\n",
    "tfidf(t, d, D) = tf(t, d) * idf(t, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb57657b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_tf = log(term_occurences_in_doc) - log(num_terms_in_doc) # Log probability of a particular term in a particular document\n",
    "\n",
    "log_log_idf = log(log(total_num_docs) - log(num_docs_containing_term)) # Log of the log probability of a particular term \n",
    "#occurring at least once in a document—the first log is to linearize the IDF (compensate for Zipf’s Law)\n",
    "\n",
    "log_tf_idf = log_tf + log_idf # Log TF-IDF is the log of the product of TF and IDF or the sum of the logs of TF and IDF.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbc03c8",
   "metadata": {},
   "source": [
    "##### Relevance ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1033dffe",
   "metadata": {},
   "source": [
    "As you saw earlier, we can easily compare two vectors and get their similarity, but wd have since learned that merely counting words isn’t as descriptive as using their TFIDF.\n",
    "Therefore, in each document vector let’s replace each word’s word_count with the word’s TF-IDF. Now our vectors will more thoroughly reflect the meaning, or topic, of the document, as shown in this Harry example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "da8e504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_tfidf_vectors = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44563e03",
   "metadata": {},
   "source": [
    "we need to copy the zero_vector to create a new, separate object. Otherwise we’d end up overwriting the same object/vector each time through the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "36a6bb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    vec = copy.copy(zero_vector)\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "    for key, value in token_counts.items():\n",
    "        docs_containing_key = 0\n",
    "        for _doc in docs:\n",
    "            if key in _doc:\n",
    "                docs_containing_key += 1\n",
    "            tf = value / len(lexicon)\n",
    "            if docs_containing_key:\n",
    "                idf = len(docs) / docs_containing_key\n",
    "        else:\n",
    "            idf = 0\n",
    "        vec[key] = tf * idf\n",
    "        document_tfidf_vectors.append(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1724f2c",
   "metadata": {},
   "source": [
    "Two vectors are considered similar if their cosine similarity is high, so you can find two similar vectors near each other if they minimize:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42eaccb",
   "metadata": {},
   "source": [
    "cos Θ = A · B/ |A| |B|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56895ab2",
   "metadata": {},
   "source": [
    "##### The last step is then to find the documents whose vectors have the highest cosine similarities to the query and return those as the search results. If you take your three documents about Harry, and make the query “How long does it take to get to the store?” as shown here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4a92343b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How long does it take to get to the store?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "31f7125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vec = copy.copy(zero_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ce3090e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vec = copy.copy(zero_vector) # copy.copy() ensures you’re dealing with separate objects, not multiple references to the same object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9a82fe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(query.lower())\n",
    "token_counts = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7e08cd54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key, value in token_counts.items():\n",
    "    docs_containing_key = 0\n",
    "    for _doc in docs:\n",
    "        if key in _doc.lower():\n",
    "            docs_containing_key += 1\n",
    "    if docs_containing_key == 0:  #You didn’t find that token in the lexicon, so go to the next key.\n",
    "            continue\n",
    "            tf = value / len(tokens)\n",
    "            idf = len(documents) / docs_containing_key\n",
    "            query_vec[key] = tf * idf\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2fbbdaba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [134]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcosine_sim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocument_tfidf_vectors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [60]\u001b[0m, in \u001b[0;36mcosine_sim\u001b[1;34m(vec1, vec2)\u001b[0m\n\u001b[0;32m      8\u001b[0m mag_1 \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28msum\u001b[39m([x\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m vec1]))\n\u001b[0;32m      9\u001b[0m mag_2 \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28msum\u001b[39m([x\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m vec2]))\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdot_prod\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mmag_1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmag_2\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "cosine_sim(query_vec, document_tfidf_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fac5fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8aaf1db9",
   "metadata": {},
   "source": [
    "##### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f9e375d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\emz\\anaconda3\\lib\\site-packages (from sklearn) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\emz\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.21.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\emz\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\emz\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\emz\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=1e92b2f53c1332c39616060d378d24124bb9d2e78a28da09ce914b5a4a9fe268\n",
      "  Stored in directory: c:\\users\\emz\\appdata\\local\\pip\\cache\\wheels\\e4\\7b\\98\\b6466d71b8d738a0c547008b9eb39bf8676d1ff6ca4b22af1c\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000010713315A90>: Failed to establish a new connection: [Errno 11004] getaddrinfo failed')': /packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n"
     ]
    }
   ],
   "source": [
    "#!pip install scipy\n",
    "#!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def8fe0e",
   "metadata": {},
   "source": [
    "##### The sklearn TF-IDF class is a model with .fit() and .transform() methods that comply with the sklearn API for all machine learning models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f4f6d415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "36b25fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "cf61d39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e6b3a546",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vectorizer.fit_transform(corpus) #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73654bc6",
   "metadata": {},
   "source": [
    "The TFIDFVectorizer model produces a sparse numpy matrix, because a TF-IDF matrix usually contains mostly zeros, since most documents use a small portion of the total words in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4cb478ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16 0.   0.48 0.21 0.21 0.   0.25 0.21 0.   0.   0.   0.21 0.   0.64\n",
      "  0.21 0.21]\n",
      " [0.37 0.   0.37 0.   0.   0.37 0.29 0.   0.37 0.37 0.   0.   0.49 0.\n",
      "  0.   0.  ]\n",
      " [0.   0.75 0.   0.   0.   0.29 0.22 0.   0.29 0.29 0.38 0.   0.   0.\n",
      "  0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "print(model.todense().round(2)) ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae7486b",
   "metadata": {},
   "source": [
    "###### ## The .todense() method converts a sparse matrix back into a regular numpy matrix (filling in the gaps with zeros) for your viewing pleasure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b107c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
