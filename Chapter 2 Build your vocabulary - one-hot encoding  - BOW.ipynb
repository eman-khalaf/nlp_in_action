{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07859eb5",
   "metadata": {},
   "source": [
    "### - np of zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19a654c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cf0228d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"Thomas Jefferson began building Monticello at the age of 26.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281c2e89",
   "metadata": {},
   "source": [
    "#### Simple Tokenize using split function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2df0935b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '26.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2a79e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '26.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str.split(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19897ec0",
   "metadata": {},
   "source": [
    "###### Main problem here that it dependa only on the white space with out considering the puncituaton."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceb83b9",
   "metadata": {},
   "source": [
    "#### For now, let’s forge ahead with your imperfect tokenizer. You’ll deal with punctuation and other challenges later. With a bit more Python, you can create a numerical vector representation for each word. These vectors are called one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bd69df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sequence = str.split(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae57371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(token_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ead92ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['26.',\n",
       " 'Jefferson',\n",
       " 'Monticello',\n",
       " 'Thomas',\n",
       " 'age',\n",
       " 'at',\n",
       " 'began',\n",
       " 'building',\n",
       " 'of',\n",
       " 'the']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12622c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'26., Jefferson, Monticello, Thomas, age, at, began, building, of, the'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e65c68d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(token_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "244e6110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7d3502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f18aa3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bd940e",
   "metadata": {},
   "source": [
    "###### Creating empty table as wide as the count of unique vocabulary terms and as high as the length of the document, 10 rows by 10 columns.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24699783",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_vectors = np.zeros((num_tokens, vocab_size), int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c4fd177",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, word in enumerate(token_sequence): \n",
    "    onehot_vectors[i, vocab.index(word)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436a20f3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3a0d40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'26. Jefferson Monticello Thomas age at began building of the'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9e3a14c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec084bb",
   "metadata": {},
   "source": [
    "If you have trouble quickly reading all those ones and zeros, you’re not alone. Pandas DataFrames can help make this a little easier on the eyes and more informative. A DataFrame keeps track of labels for each column, allowing you to label each column\n",
    "in our table with the token or word it represents. A DataFrame can also keep track of labels for each row in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49c13f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a48e4df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>26.</th>\n",
       "      <th>Jefferson</th>\n",
       "      <th>Monticello</th>\n",
       "      <th>Thomas</th>\n",
       "      <th>age</th>\n",
       "      <th>at</th>\n",
       "      <th>began</th>\n",
       "      <th>building</th>\n",
       "      <th>of</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   26.  Jefferson  Monticello  Thomas  age  at  began  building  of  the\n",
       "0    0          0           0       1    0   0      0         0   0    0\n",
       "1    0          1           0       0    0   0      0         0   0    0\n",
       "2    0          0           0       0    0   0      1         0   0    0\n",
       "3    0          0           0       0    0   0      0         1   0    0\n",
       "4    0          0           1       0    0   0      0         0   0    0\n",
       "5    0          0           0       0    0   1      0         0   0    0\n",
       "6    0          0           0       0    0   0      0         0   0    1\n",
       "7    0          0           0       0    1   0      0         0   0    0\n",
       "8    0          0           0       0    0   0      0         0   1    0\n",
       "9    1          0           0       0    0   0      0         0   0    0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(onehot_vectors, columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8a10c2",
   "metadata": {},
   "source": [
    "One-hot vectors are super-sparse, containing only one nonzero value in each row vector. So we can make that table of one-hot row vectors even prettier by replacing  zeros with blanks. Don’t do this with any DataFrame you intend to use in your machine learning pipeline, because it’ll create a lot of non-numerical objects within your numpy array, mucking up the math. But if you just want to see how this one-hot vector sequence is like a mechanical music box cylinder, or a player piano drum, the following listing can be a handy view of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b33a815",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(onehot_vectors, columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa01a96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df == 0] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5046e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>26.</th>\n",
       "      <th>Jefferson</th>\n",
       "      <th>Monticello</th>\n",
       "      <th>Thomas</th>\n",
       "      <th>age</th>\n",
       "      <th>at</th>\n",
       "      <th>began</th>\n",
       "      <th>building</th>\n",
       "      <th>of</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  26. Jefferson Monticello Thomas age at began building of the\n",
       "0                               1                             \n",
       "1             1                                               \n",
       "2                                            1                \n",
       "3                                                     1       \n",
       "4                        1                                    \n",
       "5                                      1                      \n",
       "6                                                            1\n",
       "7                                   1                         \n",
       "8                                                        1    \n",
       "9   1                                                         "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495888b8",
   "metadata": {},
   "source": [
    " ##### Creating BOW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ac098ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_bow = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5df293ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in sentence.split():\n",
    "     sentence_bow[token] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac8160a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('26.', 1),\n",
       " ('Jefferson', 1),\n",
       " ('Monticello', 1),\n",
       " ('Thomas', 1),\n",
       " ('age', 1),\n",
       " ('at', 1),\n",
       " ('began', 1),\n",
       " ('building', 1),\n",
       " ('of', 1),\n",
       " ('the', 1)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(sentence_bow.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20d427b",
   "metadata": {},
   "source": [
    "you might also notice that using a dict (or any paired mapping of words to their 0/1 values) to store a binary vector \n",
    "shouldn’t waste much space. Using a dictionary to represent your vector ensures that it only has to store a 1 when any\n",
    "one of the thousands, or even millions, of possible words in your dictionary appear in a particular document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20cab3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(pd.Series(dict([(token, 1) for token in\n",
    "    sentence.split()])), columns=['sent']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e208df96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Thomas</th>\n",
       "      <th>Jefferson</th>\n",
       "      <th>began</th>\n",
       "      <th>building</th>\n",
       "      <th>Monticello</th>\n",
       "      <th>at</th>\n",
       "      <th>the</th>\n",
       "      <th>age</th>\n",
       "      <th>of</th>\n",
       "      <th>26.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Thomas  Jefferson  began  building  Monticello  at  the  age  of  26.\n",
       "sent       1          1      1         1           1   1    1    1   1    1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe59ea5",
   "metadata": {},
   "source": [
    "Let’s add a few more texts to your corpus to see how a DataFrame stacks up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "71813e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = \"\"\"Thomas Jefferson began building Monticello at the age of 26.\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "305eed77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences +=\"\"\"Construction was done mostly by local masons and carpenters.\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0ebab3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences += \"He moved into the South Pavilion in 1770.\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c6ee97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences += \"\"\"Turning Monticello into a neoclassical masterpiece was Jefferson's obsession.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "903cc708",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "87ae4d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sent in enumerate(sentences.split('\\n')):\n",
    "    corpus['sent{}'.format(i)] = dict((tok, 1) for tok in sent.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9cf229f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame.from_records(corpus).fillna(0).astype(int).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dc6d0e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Thomas</th>\n",
       "      <th>Jefferson</th>\n",
       "      <th>began</th>\n",
       "      <th>building</th>\n",
       "      <th>Monticello</th>\n",
       "      <th>at</th>\n",
       "      <th>the</th>\n",
       "      <th>age</th>\n",
       "      <th>of</th>\n",
       "      <th>26.</th>\n",
       "      <th>Construction</th>\n",
       "      <th>was</th>\n",
       "      <th>done</th>\n",
       "      <th>mostly</th>\n",
       "      <th>by</th>\n",
       "      <th>local</th>\n",
       "      <th>masons</th>\n",
       "      <th>and</th>\n",
       "      <th>carpenters.</th>\n",
       "      <th>He</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Thomas  Jefferson  began  building  Monticello  at  the  age  of  26.  \\\n",
       "sent0       1          1      1         1           1   1    1    1   1    1   \n",
       "sent1       0          0      0         0           0   0    0    0   0    0   \n",
       "sent2       0          0      0         0           0   0    1    0   0    0   \n",
       "sent3       0          0      0         0           1   0    0    0   0    0   \n",
       "\n",
       "       Construction  was  done  mostly  by  local  masons  and  carpenters.  \\\n",
       "sent0             0    0     0       0   0      0       0    0            0   \n",
       "sent1             1    1     1       1   1      1       1    1            1   \n",
       "sent2             0    0     0       0   0      0       0    0            0   \n",
       "sent3             0    1     0       0   0      0       0    0            0   \n",
       "\n",
       "       He  \n",
       "sent0   0  \n",
       "sent1   0  \n",
       "sent2   1  \n",
       "sent3   0  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3[df3.columns[:20]] # This shows only the first 20 tokens(DataFrame columns), to avoid wrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8530f5c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cbc26d7",
   "metadata": {},
   "source": [
    "Example dot product calculation\n",
    "dot product is simply the sum of the multiplication of the x1*2,y1*2,z1*2>>>> A = (x1, y1, z1 ) , B = (x2, y2, z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "14245a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EMZ\\AppData\\Local\\Temp\\ipykernel_14468\\3385979122.py:1: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n",
      "  v1 = pd.np.array([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "v1 = pd.np.array([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bdd52c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "v2 = np.array([2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dc1bca39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1.dot(v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fd3f08ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(v1 * v2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a49e2250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([x1 * x2 for x1, x2 in zip(v1, v2)]) #You shouldn’t iterate through vectors this way unless you want to slow down your pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628d4935",
   "metadata": {},
   "source": [
    "#### Measuring bag-of-words overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d5eba16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "970ae5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent0</th>\n",
       "      <th>sent1</th>\n",
       "      <th>sent2</th>\n",
       "      <th>sent3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Thomas</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jefferson</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>began</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>building</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Monticello</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>at</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26.</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Construction</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>was</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>done</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mostly</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>local</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>masons</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carpenters.</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>He</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moved</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>into</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pavilion</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Turning</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neoclassical</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>masterpiece</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jefferson's</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obsession.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              sent0  sent1  sent2  sent3\n",
       "Thomas            1      0      0      0\n",
       "Jefferson         1      0      0      0\n",
       "began             1      0      0      0\n",
       "building          1      0      0      0\n",
       "Monticello        1      0      0      1\n",
       "at                1      0      0      0\n",
       "the               1      0      1      0\n",
       "age               1      0      0      0\n",
       "of                1      0      0      0\n",
       "26.               1      0      0      0\n",
       "Construction      0      1      0      0\n",
       "was               0      1      0      1\n",
       "done              0      1      0      0\n",
       "mostly            0      1      0      0\n",
       "by                0      1      0      0\n",
       "local             0      1      0      0\n",
       "masons            0      1      0      0\n",
       "and               0      1      0      0\n",
       "carpenters.       0      1      0      0\n",
       "He                0      0      1      0\n",
       "moved             0      0      1      0\n",
       "into              0      0      1      1\n",
       "South             0      0      1      0\n",
       "Pavilion          0      0      1      0\n",
       "in                0      0      1      0\n",
       "1770.             0      0      1      0\n",
       "Turning           0      0      0      1\n",
       "a                 0      0      0      1\n",
       "neoclassical      0      0      0      1\n",
       "masterpiece       0      0      0      1\n",
       "Jefferson's       0      0      0      1\n",
       "obsession.        0      0      0      1"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aed4e803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.sent0.dot(df4.sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "beb6b337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.sent0.dot(df4.sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "502bcb53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.sent0.dot(df4.sent3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e46f543",
   "metadata": {},
   "source": [
    "Here’s one way to find the word that is shared by sent0 and sent3, the word that gave you that last dot product of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "987da049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Monticello', 1)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k, v) for (k, v) in (df4.sent0 & df4.sent3).items() if v]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c263d633",
   "metadata": {},
   "source": [
    "This is our first vector space model (VSM) of natural language documents (sentences). Not only are dot products possible, but other vector operations are defined for these bag-of-word vectors: addition, subtraction, OR, AND, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa3ae0f",
   "metadata": {},
   "source": [
    "#### Tokenize the Monticello sentence with a regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f882919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9c86c40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"Thomas Jefferson began building Monticello at the age of 26.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "31f5db76",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = re.split(r'[-\\s.,;!?]+', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2ed7feba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '26',\n",
       " '']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acbfeb1",
   "metadata": {},
   "source": [
    "This splits the sentence on whitespace or punctuation that occurs at least once (note the '+' after the closing square bracket in the regular expression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "803c5312",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r\"([-\\s.,;!?])+\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "841e81f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_b = pattern.split(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2ec8b569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " ' ',\n",
       " 'Jefferson',\n",
       " ' ',\n",
       " 'began',\n",
       " ' ',\n",
       " 'building',\n",
       " ' ',\n",
       " 'Monticello',\n",
       " ' ',\n",
       " 'at',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'age',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " '26',\n",
       " '.',\n",
       " '']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "68adcbd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '26']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in tokens_b if x and x not in '- \\t\\n.,;!?'] # to remove white space and puncituation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8f86cb",
   "metadata": {},
   "source": [
    "If you want practice withlambda and filter(), use\n",
    "list(filter(lambda x: x if x and x not in '- \\t\\n.,;!?' else None, tokens))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "642cbd90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '26']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: x if x and x not in '- \\t\\n.,;!?' else None, tokens_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5547c77d",
   "metadata": {},
   "source": [
    "we can use the NLTK function RegexpTokenizer to replicate your simple tokenizer example like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "adb28411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ede0c0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a83a51e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+|$[0-9.]+|\\S+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a5430238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas',\n",
       " 'Jefferson',\n",
       " 'began',\n",
       " 'building',\n",
       " 'Monticello',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '26',\n",
       " '.']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb95c66f",
   "metadata": {},
   "source": [
    "An even better tokenizer is the Treebank Word Tokenizer from the NLTK package. It incorporates a variety of common rules for English word tokenization. For example, it separates phrase-terminating punctuation (?!.;,) from adjacent tokens and retains\n",
    "decimal numbers containing a period as a single token. In addition it contains rules for English contractions. For example “don’t” is tokenized as [\"do\", \"n’t\"]. This tokenization will help with subsequent steps in the NLP pipeline, such as stemming. You can find all the rules for the Treebank Tokenizer at http://www.nltk.org/api/nltk.tokenize.html#modulenltk.tokenize.treebank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "be8ded95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "767b8c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_b = \"\"\"Monticello wasn't designated as UNESCO World Heritage Site until 1987.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "55dbe91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "559c73ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Monticello',\n",
       " 'was',\n",
       " \"n't\",\n",
       " 'designated',\n",
       " 'as',\n",
       " 'UNESCO',\n",
       " 'World',\n",
       " 'Heritage',\n",
       " 'Site',\n",
       " 'until',\n",
       " '1987',\n",
       " '.']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(sentence_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d68045a",
   "metadata": {},
   "source": [
    "#### Tokenize informal text from social networks such as Twitter and Facebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762a7bdb",
   "metadata": {},
   "source": [
    "The NLTK library includes a tokenizer—casual_tokenize—that was built to deal with short, informal, emoticon-laced texts from social networks where grammar and spelling conventions vary widely.\n",
    "The casual_tokenize function allows us to strip usernames and reduce the number of repeated characters within a token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "844b9fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.casual import casual_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9630f8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"\"\"RT @TJMonticello Best day everrrrrrr at Monticello. Awesommmmmmeeeeeeee day :*)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b7f25cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT',\n",
       " '@TJMonticello',\n",
       " 'Best',\n",
       " 'day',\n",
       " 'everrrrrrr',\n",
       " 'at',\n",
       " 'Monticello',\n",
       " '.',\n",
       " 'Awesommmmmmeeeeeeee',\n",
       " 'day',\n",
       " ':*)']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "casual_tokenize(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "62d191d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT',\n",
       " 'Best',\n",
       " 'day',\n",
       " 'everrr',\n",
       " 'at',\n",
       " 'Monticello',\n",
       " '.',\n",
       " 'Awesommmeee',\n",
       " 'day',\n",
       " ':*)']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "casual_tokenize(message, reduce_len=True, strip_handles=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b53485",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be490af6",
   "metadata": {},
   "source": [
    "### Extending your vocabulary with n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e485062c",
   "metadata": {},
   "source": [
    "An n-gram is a sequence containing up to n elements that have been extracted from a sequence of those elements, usually a string. In general the “elements” of an n-gram can be characters, syllables, words, or even symbols like “A,” “T,” “G,” and “C” used to represent a DNA sequence. we’re only interested in n-grams of words, not characters.7 So in this book, when we say 2-gram, we mean a pair of words, like “ice cream.” When we say 3-gram, we mean a triplet of words like “beyond the pale” or “Johann Sebastian Bach”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadcb21c",
   "metadata": {},
   "source": [
    "n-grams don’t have to mean something special together, like compound words. They merely have to be frequent enough together to catch the attention of your token counters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05a3fa0",
   "metadata": {},
   "source": [
    "In the next chapter, we show you how to recognize which of these n-grams contain the most information relative to the others, which you can use to reduce the number of tokens (n-grams) your NLP pipeline has to keep track of. Otherwise it would have to store and maintain a list of every single word sequence it came across. This prioritization of n-grams will help it recognize “Thomas Jefferson” and “ice cream,” without paying particular attention to “Thomas Smith” or “ice shattered.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb7f721",
   "metadata": {},
   "source": [
    "##### Let’s use your original sentence about Thomas Jefferson to show what a 2-gram tokenizer should output, so you know what you’re trying to build:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "03fbfc8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenize_2grams' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [89]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtokenize_2grams\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThomas Jefferson began building Monticello at the age of 26.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenize_2grams' is not defined"
     ]
    }
   ],
   "source": [
    "tokenize_2grams(\"Thomas Jefferson began building Monticello at the age of 26.\") # p49"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479d1c6f",
   "metadata": {},
   "source": [
    "n-grams are one of the ways to maintain context information as data passes through your pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97aec83",
   "metadata": {},
   "source": [
    "###### this is the n-gram tokenizer from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0bf230ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9f6e0ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Thomas', 'Jefferson'),\n",
       " ('Jefferson', 'began'),\n",
       " ('began', 'building'),\n",
       " ('building', 'Monticello'),\n",
       " ('Monticello', 'at'),\n",
       " ('at', 'the'),\n",
       " ('the', 'age'),\n",
       " ('age', 'of'),\n",
       " ('of', '26'),\n",
       " ('26', '')]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(tokens, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a644afca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Thomas', 'Jefferson', 'began'),\n",
       " ('Jefferson', 'began', 'building'),\n",
       " ('began', 'building', 'Monticello'),\n",
       " ('building', 'Monticello', 'at'),\n",
       " ('Monticello', 'at', 'the'),\n",
       " ('at', 'the', 'age'),\n",
       " ('the', 'age', 'of'),\n",
       " ('age', 'of', '26'),\n",
       " ('of', '26', '')]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(tokens, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c99bc2d",
   "metadata": {},
   "source": [
    "The n-grams are provided in the previous listing as tuples, but they can easily be joined together if you’d like all the tokens in your pipeline to be strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c25cea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_grams = list(ngrams(tokens, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "08ab4c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thomas Jefferson',\n",
       " 'Jefferson began',\n",
       " 'began building',\n",
       " 'building Monticello',\n",
       " 'Monticello at',\n",
       " 'at the',\n",
       " 'the age',\n",
       " 'age of',\n",
       " 'of 26',\n",
       " '26 ']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\" \".join(x) for x in two_grams]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ed01ae",
   "metadata": {},
   "source": [
    "n-grams are usually filtered out if they occur too often. For example, if a token or n-gram occurs in more than 25% of all the documents in your corpus, you usually ignore it. This is equivalent to the “stop words” filter in the coin-sorting machine of chapter 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67b8c57",
   "metadata": {},
   "source": [
    "##### STOP WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a03e38",
   "metadata": {},
   "source": [
    "Stop words are common words in any language that occur with a high frequency but carry much less substantive information about the meaning of a phrase. \n",
    "A more comprehensive list of stop words for various languages can be found in NLTK’s corpora (https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/stopwords.zip)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795cfec2",
   "metadata": {},
   "source": [
    "If we do decide to arbitrarily filter out a set of stop words during tokenization, a Python list comprehension is sufficient. Here we take a few stop words and ignore them when we iterate through your token list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ca3d6cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['a', 'an', 'the', 'on', 'of', 'off', 'this', 'is']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fd6351a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['the', 'house', 'is', 'on', 'fire']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8d8062f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_without_stopwords = [x for x in tokens if x not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b1312a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['house', 'fire']\n"
     ]
    }
   ],
   "source": [
    "print(tokens_without_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8a1d2169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['house', 'fire']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_without_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ca02c4",
   "metadata": {},
   "source": [
    "#### NLTK list of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c7bac0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "93cb5260",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\EMZ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d00a184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6e2bacaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "26638ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "726f3639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'a', 's', 't', 'd', 'm', 'o', 'y']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sw for sw in stop_words if len(sw) == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52c1cfc",
   "metadata": {},
   "source": [
    "These one-letter stop words are even more curious, but they make sense if you’ve used the NLTK tokenizer and Porter stemmer a lot. These single-letter tokens pop up a lot when contractions are split and stemmed using NLTK tokenizers and stemmers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d3a12c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words2 = nltk.corpus.stopwords.words('arabic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a5921feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "754"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3d390f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['إذ', 'إذا', 'إذما', 'إذن', 'أف', 'أقل', 'أكثر']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words2[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b34cbd93",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['الذين',\n",
       " 'إليكم',\n",
       " 'إليكن',\n",
       " 'أنتما',\n",
       " 'أولاء',\n",
       " 'أولئك',\n",
       " 'أينما',\n",
       " 'بماذا',\n",
       " 'تلكما',\n",
       " 'حيثما',\n",
       " 'ذلكما',\n",
       " 'ذواتا',\n",
       " 'ذواتي',\n",
       " 'كأنما',\n",
       " 'كيفما',\n",
       " 'لستما',\n",
       " 'لكنما',\n",
       " 'لكيلا',\n",
       " 'ليستا',\n",
       " 'ليسوا',\n",
       " 'هاتان',\n",
       " 'هاتين',\n",
       " 'هاهنا',\n",
       " 'هنالك',\n",
       " 'هؤلاء',\n",
       " 'هيهات',\n",
       " 'والذي',\n",
       " 'يناير',\n",
       " 'أبريل',\n",
       " 'يونيو',\n",
       " 'يوليو',\n",
       " 'أغسطس',\n",
       " 'جانفي',\n",
       " 'فيفري',\n",
       " 'أفريل',\n",
       " 'كانون',\n",
       " 'نيسان',\n",
       " 'أيلول',\n",
       " 'تشرين',\n",
       " 'دولار',\n",
       " 'دينار',\n",
       " 'سنتيم',\n",
       " 'اثنان',\n",
       " 'ثلاثة',\n",
       " 'أربعة',\n",
       " 'ثماني',\n",
       " 'اثنين',\n",
       " 'إياها',\n",
       " 'إياهم',\n",
       " 'إياهن',\n",
       " 'إياكم',\n",
       " 'إياكن',\n",
       " 'إيانا',\n",
       " 'تانِك',\n",
       " 'هَذِه',\n",
       " 'هَذِي',\n",
       " 'الألى',\n",
       " 'كأيّن',\n",
       " 'آمينَ',\n",
       " 'أُفٍّ',\n",
       " 'أُفٍّ',\n",
       " 'أمامك',\n",
       " 'أوّهْ',\n",
       " 'إليكَ',\n",
       " 'رويدك',\n",
       " 'سرعان',\n",
       " 'شتانَ',\n",
       " 'واهاً',\n",
       " 'خبَّر',\n",
       " 'نبَّا',\n",
       " 'طالما',\n",
       " 'لكنَّ',\n",
       " 'رُبَّ',\n",
       " 'كلَّا',\n",
       " 'لعلَّ',\n",
       " 'لكنَّ',\n",
       " 'لكنَّ',\n",
       " 'تلقاء',\n",
       " 'سبحان',\n",
       " 'مئتان',\n",
       " 'ستمئة',\n",
       " 'عشرون',\n",
       " 'خمسون',\n",
       " 'سبعون',\n",
       " 'تسعون',\n",
       " 'عشرين',\n",
       " 'خمسين',\n",
       " 'سبعين',\n",
       " 'تسعين',\n",
       " 'خلافا',\n",
       " 'صراحة',\n",
       " 'عيانا',\n",
       " 'غالبا',\n",
       " 'فرادى',\n",
       " 'قاطبة',\n",
       " 'كثيرا',\n",
       " 'أيّان',\n",
       " 'كلّما',\n",
       " 'ارتدّ',\n",
       " 'انقلب',\n",
       " 'تبدّل',\n",
       " 'تحوّل',\n",
       " 'مادام',\n",
       " 'مازال',\n",
       " 'مافتئ',\n",
       " 'ابتدأ',\n",
       " 'انبرى']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sw for sw in stop_words2 if len(sw) == 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c968afb5",
   "metadata": {},
   "source": [
    "Depending on how much natural language information we want to discard, we can take the union or the intersection of multiple stop word lists for your pipeline. Here’s a comparison of sklearn stop words (version 0.19.2) and nltk stop words (version 3.2.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f34466fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e3951616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sklearn_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0ad6d329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fc41a935",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'union'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [120]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mstop_words\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munion\u001b[49m(sklearn_stop_words))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'union'"
     ]
    }
   ],
   "source": [
    "len(stop_words.union(sklearn_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d9823e47",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'intersection'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [121]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mstop_words\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintersection\u001b[49m(sklearn_stop_words))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'intersection'"
     ]
    }
   ],
   "source": [
    "len(stop_words.intersection(sklearn_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3c04fde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for another way to that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bfb9e3",
   "metadata": {},
   "source": [
    "### Normalizing the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ff25ac",
   "metadata": {},
   "source": [
    "Case folding is when you consolidate multiple “spellings” of a word that differ only in their capitalization.\n",
    "Normalizing word and character capitalization is one way to reduce your vocabulary size and generalize your NLP pipeline. It helps you consolidate words that are intended to mean the same thing (and be spelled the same way) under a single token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cac9b7",
   "metadata": {},
   "source": [
    "Often capitalization is used to indicate that a word is a proper noun, the name of a person, place, or thing. You’ll want to be able to recognize proper nouns as distinct from other words, if named entity recognition is important to your pipeline. However, if tokens aren’t case normalized, your vocabulary will be approximately twice as large, consume twice as\n",
    "much memory and processing time, and might increase the amount of training data you need to label for your machine learning pipeline to converge to an accurate, general solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f7efdc",
   "metadata": {},
   "source": [
    "Just as in any other machine learning pipeline, your labeled dataset used for training must be “representative” of the space of all possible feature vectors your model must deal with, including variations in capitalization. For 100,000-D bag-ofwords\n",
    "vectors, you usually must have 100,000 labeled examples, and sometimes even more than that, to train a supervised machine learning pipeline without overfitting. In some situations, cutting your vocabulary size by half can be worth the loss of information content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c38e805",
   "metadata": {},
   "source": [
    "##### In Python, you can easily normalize the capitalization of your tokens with a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c29fe36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['House', 'Visitor', 'Center']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "643b3fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_tokens = [x.lower() for x in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3268a5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['house', 'visitor', 'center']\n"
     ]
    }
   ],
   "source": [
    "print(normalized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71287809",
   "metadata": {},
   "source": [
    "And if you’re certain that you want to normalize the case for an entire document, you can lower() the text string in one operation, before tokenization. But this will prevent advanced tokenizers that can split camel case words like “WordPerfect,” “FedEx,” or “stringVariableName.”10 Maybe you want WordPerfect to be it’s own unique thing (token), or maybe you want to reminisce about a more perfect word processing era. It’s up to you to decide when and how to apply case folding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aa21da",
   "metadata": {},
   "source": [
    "With case normalization, you are attempting to return these tokens to their “normal” state before grammar rules and their position in a sentence affected their capitalization. The simplest and most common way to normalize the case of a text string is to lowercase all the characters with a function like Python’s built-in str.lower()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8cdf69",
   "metadata": {},
   "source": [
    "but\n",
    "Lowercasing on the first word in a sentence preserves the meaning of proper nouns in the middle of a sentence, like “Joe” and “Smith” in “Joe Smith.” And it properly groups words together that belong together, because they’re only capitalized when they are at the beginning of a sentence, since they aren’t proper nouns. This prevents “Joe” from being confused with “coffee” (“joe”)12 during tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07d9076",
   "metadata": {},
   "source": [
    "this careful approach to case normalization, where you lowercase words only at the start of a sentence, you will still introduce capitalization errors for the rare proper nouns that start a sentence. “Joe Smith, the word smith, with a cup of joe.” will produce a different set of tokens than “Smith the word with a cup of joe, Joe Smith.” And you may not want that. In addition, case normalization is useless for languages that don’t have a concept of capitalization. To avoid this potential loss of information, many NLP pipelines don’t normalize for case at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95da361e",
   "metadata": {},
   "source": [
    "The best way to find out what works is to try several different approaches, and see which approach gives you the best performance for the objectives of your NLP project. By generalizing your model to work with text that has odd capitalization, case normalization can reduce overfitting for your machine learning pipeline. Case normalization is particularly useful for a search engine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fa33e9",
   "metadata": {},
   "source": [
    "##### STEMMING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667d60bd",
   "metadata": {},
   "source": [
    "Another common vocabulary normalization technique is to eliminate the small meaning differences of pluralization or possessive endings of words, or even various verb forms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e68c533",
   "metadata": {},
   "source": [
    "Stemming removes suffixes from words in an attempt to combine words with similar meanings together under their common stem. A stem isn’t required to be a properly spelled word, but merely a token, or label, representing several possible spellings of a word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348278ab",
   "metadata": {},
   "source": [
    "In machine learning this is referred to as dimension reduction. It helps generalize your language model, enabling the model to behave identically for all the words included in a stem. So, as long as the application doesn’t require the machine to distinguish between “house” and “houses,” this stem will reduce the programming or dataset size by half or even more, depending on the aggressiveness of the stemmer of choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d855f12",
   "metadata": {},
   "source": [
    "Stemming is important for keyword search or information retrieval. It allows we to search for “developing houses in Portland” and get web pages or documents that use both the word “house” and “houses” and even the word “housing,” because these words\n",
    "are all stemmed to the “hous” token. Likewise we might receive pages with the words “developer” and “development” rather than “developing,” because all these words typically reduce to the stem “develop.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0e2edf",
   "metadata": {},
   "source": [
    "##### Here’s a simple stemmer implementation in pure Python that can handle trailing S’s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "79edd4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(phrase):\n",
    "    return ' '.join([re.findall('^(.*ss|.*?)(s)?$', word)[0][0].strip(\"'\") for word in phrase.lower().split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "75146c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'house'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem('houses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "366b2ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doctor house call'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem(\"Doctor House's calls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76834d12",
   "metadata": {},
   "source": [
    "Two of the most popular stemming algorithms are the Porter and Snowball stemmers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9fd2cefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e84b410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3762e8e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dish washer wash dish'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([stemmer.stem(w).strip(\"'\") for w in \"dish washer's washed dishes\".split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b91550d",
   "metadata": {},
   "source": [
    "Notice that the Porter stemmer, like the regular expression stemmer, retains the trailing apostrophe (unless you explicitly strip it), which ensures that possessive words will be distinguishable from nonpossessive words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074f6ecb",
   "metadata": {},
   "source": [
    " # side note \n",
    "    More on the Porter stemmer\n",
    "Julia Menchavez has graciously shared her translation of Porter’s original stemmer algorithm into pure Python (https://github.com/jedijulia/porter-stemmer/blob/master/stemmer.py). If you are ever tempted to develop your own stemmer, consider these 300 lines of code and the lifetime of refinement that Porter put into them. There are eight steps to the Porter stemmer algorithm: 1a, 1b, 1c, 2, 3, 4, 5a, and 5b. Step 1a is a bit like your regular expression for dealing with trailing S’s:\n",
    "a) This is a trivially abbreviated version of Julia Menchavez’s implementation of porter-stemmer on GitHub\n",
    "(https://github.com/jedijulia/porter-stemmer/blob/master/stemmer.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79d75b8",
   "metadata": {},
   "source": [
    "#### LEMMATIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9abd8f9",
   "metadata": {},
   "source": [
    "This more extensive normalization down to the semantic root of a word—its lemma—is called lemmatization. Any NLP pipeline that\n",
    "wants to “react” the same for multiple different spellings of the same basic root word can benefit from a lemmatizer. It reduces the number of words you have to respond to, the dimensionality of your language model. Using it can make your model more general, but it can also make your model less precise, because it will treat all spelling variations of a given root word the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96281bfa",
   "metadata": {},
   "source": [
    "while working through this section, think about words where lemmatization would drastically alter the meaning of a word, perhaps even inverting its meaning and producing the opposite of the intended response from your pipeline. This scenario is called spoofing—when someone intentionally tries to elicit the wrong response from a machine learning pipeline by cleverly constructing a difficult input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7fa938",
   "metadata": {},
   "source": [
    "Lemmatization is a potentially more accurate way to normalize a word than stemming or case normalization because it takes into account a word’s meaning. A lemmatizer uses a knowledge base of word synonyms and word endings to ensure that only words that mean similar things are consolidated into a single token.\n",
    "\n",
    "Some lemmatizers use the word’s part of speech (POS) tag in addition to its spelling to help improve accuracy. The POS tag for a word indicates its role in the grammar of a phrase or sentence. For example, the noun POS is for words that refer to “people, places, or things” within a phrase. An adjective POS is for a word that modifies or describes a noun. A verb refers to an action. The POS of a word in isolation cannot be determined. The context of a word must be known for its POS to be identified. So some advanced lemmatizers can’t be run-on words in isolation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65afcee9",
   "metadata": {},
   "source": [
    "Consider the word better. Stemmers would strip the “er” ending from “better” and return the stem “bett” or “bet.” However, this would lump the word “better” with words like “betting,” “bets,” and “Bet’s,” rather than more similar words like “betterment,” “best,” or even “good” and “goods.”\n",
    "So lemmatizers are better than stemmers for most applications. Stemmers are only really used in large-scale information retrieval applications (keyword search). And if you really want the dimension reduction and recall improvement of a stemmer in your information retrieval pipeline, you should probably also use a lemmatizer right before the stemmer. Because the lemma of a word is a valid English word, stemmers work well on the output of a lemmatizer. This trick will reduce your dimensionality and increase your information retrieval recall even more than a stemmer alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bbb6aa",
   "metadata": {},
   "source": [
    "### The NLTK package provides functions for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "968f9410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\EMZ\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c32fd31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\EMZ\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "db6310bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5d86abd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "33b8490c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'better'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"better\")  # The default part of speech is “n” for noun. the NLTK lemmatizer assumes it’s a noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "71bd0ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"better\", pos=\"a\") # “a” indicates the adjective part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "78f08e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"good\", pos=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "51929419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goods'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"goods\", pos=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7a4909c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"goods\", pos=\"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "baa759ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goodness'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"goodness\", pos=\"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "88abefd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'best'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"best\", pos=\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9427bc8f",
   "metadata": {},
   "source": [
    "Unfortunately, the NLTK lemmatizer is restricted to the connections within the Princeton WordNet graph of word meanings. So the word “best” doesn’t lemmatize to the same root as “better.” This graph is also missing the connection between “goodness” and “good.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08044f5d",
   "metadata": {},
   "source": [
    "###### Porter stemmer, on the other hand, would make this connection by blindly stripping off the “ness” ending of all words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "772d4550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('goodness')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c086af52",
   "metadata": {},
   "source": [
    "\n",
    "#### USE CASES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a31b49",
   "metadata": {},
   "source": [
    "Stemmers are generally faster to compute and require less-complex code and datasets. But stemmers will make more errors and stem a far greater number of words, reducing the information content or meaning of your text much more than a lemmatizer would. Both stemmers and lemmatizers will reduce your vocabulary size and increase the ambiguity of the text. But lemmatizers do a better job retaining as much of the information content as possible based on how the word was used within the text and its intended meaning. Therefore, some NLP packages, such as spaCy, don’t provide stemming functions and only offer lemmatization methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20b5cae",
   "metadata": {},
   "source": [
    "IMPORTANT Bottom line, try to avoid stemming and lemmatization unless you have a limited amount of text that contains usages and capitalizations of the words you are interested in. And with the explosion of NLP datasets, this is rarely the case for English documents, unless your documents use a lot of jargon or are from a very small subfield of science, technology, or literature.\n",
    "Nonetheless, for languages other than English, you may still find uses for lemmatization. The Stanford information retrieval course dismisses stemming and lemmatization entirely, due to the negligible recall accuracy improvement and the significant reduction in precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a62d63",
   "metadata": {},
   "source": [
    "#### Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca3ec26",
   "metadata": {},
   "source": [
    "This sentiment analysis—measuring the sentiment of phrases or chunks of text—is a common application of NLP. In many companies it’s the main thing an NLP engineer is asked to do.\n",
    "An NLP pipeline can process a large quantity of user feedback quickly and objectively, with less chance for bias. And an NLP pipeline can output a numerical rating of the positivity or negativity or any other emotional quality of the text.\n",
    "\n",
    "Say you just want to measure the positivity or favorability of a text—how much someone likes a product or service that they\n",
    "are writing about. Say you want your NLP pipeline and sentiment analysis algorithm to output a single floating point number between -1 and +1. Your algorithm would output +1 for text with positive sentiment like, “Absolutely perfect! Love it! :-) :-) :-).” And your algorithm should output -1 for text with negative sentiment like, “Horrible! Completely useless. :(.” Your NLP pipeline could use values near 0, like say +0.1, for a statement like, “It was OK. Some good and some bad things.”\n",
    "###### There are two approaches to sentiment analysis:\n",
    " A rule-based algorithm composed by a human\n",
    "\n",
    " A machine learning model learned from data by a machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3b7820",
   "metadata": {},
   "source": [
    "For the machine learning approach, you need a lot of data, text labeled with the “right” sentiment\n",
    "score. Twitter feeds are often used for this approach because the hash tags, such as #awesome or #happy or #sarcasm, can often be used to create a “self-labeled” dataset.\n",
    "Your company may have product reviews with five-star ratings that you could associate with reviewer comments. You can use the star ratings as a numerical score for the positivity of each text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0e25a1",
   "metadata": {},
   "source": [
    "##### VADER—A rule-based sentiment analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f723f2",
   "metadata": {},
   "source": [
    "Hutto and Gilbert at GA Tech came up with one of the first successful rule-based sentiment analysis algorithms. Many NLP packages implement some form of this algorithm. The NLTK package has an implementation of the VADER algorithm in\n",
    "nltk.sentiment.vader. Hutto himself maintains the Python package vaderSentiment.\n",
    "###### To go straight to the source and use vaderSentiment here.\n",
    "pip install vaderSentiment to run the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d54162f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\emz\\anaconda3\\lib\\site-packages (from vaderSentiment) (2.27.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\emz\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\emz\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\emz\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\emz\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2021.10.8)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dc3d21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc830f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc1002e6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$:': -1.5,\n",
       " '%)': -0.4,\n",
       " '%-)': -1.5,\n",
       " '&-:': -0.4,\n",
       " '&:': -0.7,\n",
       " \"( '}{' )\": 1.6,\n",
       " '(%': -0.9,\n",
       " \"('-:\": 2.2,\n",
       " \"(':\": 2.3,\n",
       " '((-:': 2.1,\n",
       " '(*': 1.1,\n",
       " '(-%': -0.7,\n",
       " '(-*': 1.3,\n",
       " '(-:': 1.6,\n",
       " '(-:0': 2.8,\n",
       " '(-:<': -0.4,\n",
       " '(-:o': 1.5,\n",
       " '(-:O': 1.5,\n",
       " '(-:{': -0.1,\n",
       " '(-:|>*': 1.9,\n",
       " '(-;': 1.3,\n",
       " '(-;|': 2.1,\n",
       " '(8': 2.6,\n",
       " '(:': 2.2,\n",
       " '(:0': 2.4,\n",
       " '(:<': -0.2,\n",
       " '(:o': 2.5,\n",
       " '(:O': 2.5,\n",
       " '(;': 1.1,\n",
       " '(;<': 0.3,\n",
       " '(=': 2.2,\n",
       " '(?:': 2.1,\n",
       " '(^:': 1.5,\n",
       " '(^;': 1.5,\n",
       " '(^;0': 2.0,\n",
       " '(^;o': 1.9,\n",
       " '(o:': 1.6,\n",
       " \")':\": -2.0,\n",
       " \")-':\": -2.1,\n",
       " ')-:': -2.1,\n",
       " ')-:<': -2.2,\n",
       " ')-:{': -2.1,\n",
       " '):': -1.8,\n",
       " '):<': -1.9,\n",
       " '):{': -2.3,\n",
       " ');<': -2.6,\n",
       " '*)': 0.6,\n",
       " '*-)': 0.3,\n",
       " '*-:': 2.1,\n",
       " '*-;': 2.4,\n",
       " '*:': 1.9,\n",
       " '*<|:-)': 1.6,\n",
       " '*\\\\0/*': 2.3,\n",
       " '*^:': 1.6,\n",
       " ',-:': 1.2,\n",
       " \"---'-;-{@\": 2.3,\n",
       " '--<--<@': 2.2,\n",
       " '.-:': -1.2,\n",
       " '..###-:': -1.7,\n",
       " '..###:': -1.9,\n",
       " '/-:': -1.3,\n",
       " '/:': -1.3,\n",
       " '/:<': -1.4,\n",
       " '/=': -0.9,\n",
       " '/^:': -1.0,\n",
       " '/o:': -1.4,\n",
       " '0-8': 0.1,\n",
       " '0-|': -1.2,\n",
       " '0:)': 1.9,\n",
       " '0:-)': 1.4,\n",
       " '0:-3': 1.5,\n",
       " '0:03': 1.9,\n",
       " '0;^)': 1.6,\n",
       " '0_o': -0.3,\n",
       " '10q': 2.1,\n",
       " '1337': 2.1,\n",
       " '143': 3.2,\n",
       " '1432': 2.6,\n",
       " '14aa41': 2.4,\n",
       " '182': -2.9,\n",
       " '187': -3.1,\n",
       " '2g2b4g': 2.8,\n",
       " '2g2bt': -0.1,\n",
       " '2qt': 2.1,\n",
       " '3:(': -2.2,\n",
       " '3:)': 0.5,\n",
       " '3:-(': -2.3,\n",
       " '3:-)': -1.4,\n",
       " '4col': -2.2,\n",
       " '4q': -3.1,\n",
       " '5fs': 1.5,\n",
       " '8)': 1.9,\n",
       " '8-d': 1.7,\n",
       " '8-o': -0.3,\n",
       " '86': -1.6,\n",
       " '8d': 2.9,\n",
       " ':###..': -2.4,\n",
       " ':$': -0.2,\n",
       " ':&': -0.6,\n",
       " \":'(\": -2.2,\n",
       " \":')\": 2.3,\n",
       " \":'-(\": -2.4,\n",
       " \":'-)\": 2.7,\n",
       " ':(': -1.9,\n",
       " ':)': 2.0,\n",
       " ':*': 2.5,\n",
       " ':-###..': -2.5,\n",
       " ':-&': -0.5,\n",
       " ':-(': -1.5,\n",
       " ':-)': 1.3,\n",
       " ':-))': 2.8,\n",
       " ':-*': 1.7,\n",
       " ':-,': 1.1,\n",
       " ':-.': -0.9,\n",
       " ':-/': -1.2,\n",
       " ':-<': -1.5,\n",
       " ':-d': 2.3,\n",
       " ':-D': 2.3,\n",
       " ':-o': 0.1,\n",
       " ':-p': 1.5,\n",
       " ':-[': -1.6,\n",
       " ':-\\\\': -0.9,\n",
       " ':-c': -1.3,\n",
       " ':-|': -0.7,\n",
       " ':-||': -2.5,\n",
       " ':-Þ': 0.9,\n",
       " ':/': -1.4,\n",
       " ':3': 2.3,\n",
       " ':<': -2.1,\n",
       " ':>': 2.1,\n",
       " ':?)': 1.3,\n",
       " ':?c': -1.6,\n",
       " ':@': -2.5,\n",
       " ':d': 2.3,\n",
       " ':D': 2.3,\n",
       " ':l': -1.7,\n",
       " ':o': -0.4,\n",
       " ':p': 1.0,\n",
       " ':s': -1.2,\n",
       " ':[': -2.0,\n",
       " ':\\\\': -1.3,\n",
       " ':]': 2.2,\n",
       " ':^)': 2.1,\n",
       " ':^*': 2.6,\n",
       " ':^/': -1.2,\n",
       " ':^\\\\': -1.0,\n",
       " ':^|': -1.0,\n",
       " ':c': -2.1,\n",
       " ':c)': 2.0,\n",
       " ':o)': 2.1,\n",
       " ':o/': -1.4,\n",
       " ':o\\\\': -1.1,\n",
       " ':o|': -0.6,\n",
       " ':P': 1.4,\n",
       " ':{': -1.9,\n",
       " ':|': -0.4,\n",
       " ':}': 2.1,\n",
       " ':Þ': 1.1,\n",
       " ';)': 0.9,\n",
       " ';-)': 1.0,\n",
       " ';-*': 2.2,\n",
       " ';-]': 0.7,\n",
       " ';d': 0.8,\n",
       " ';D': 0.8,\n",
       " ';]': 0.6,\n",
       " ';^)': 1.4,\n",
       " '</3': -3.0,\n",
       " '<3': 1.9,\n",
       " '<:': 2.1,\n",
       " '<:-|': -1.4,\n",
       " '=)': 2.2,\n",
       " '=-3': 2.0,\n",
       " '=-d': 2.4,\n",
       " '=-D': 2.4,\n",
       " '=/': -1.4,\n",
       " '=3': 2.1,\n",
       " '=d': 2.3,\n",
       " '=D': 2.3,\n",
       " '=l': -1.2,\n",
       " '=\\\\': -1.2,\n",
       " '=]': 1.6,\n",
       " '=p': 1.3,\n",
       " '=|': -0.8,\n",
       " '>-:': -2.0,\n",
       " '>.<': -1.3,\n",
       " '>:': -2.1,\n",
       " '>:(': -2.7,\n",
       " '>:)': 0.4,\n",
       " '>:-(': -2.7,\n",
       " '>:-)': -0.4,\n",
       " '>:/': -1.6,\n",
       " '>:o': -1.2,\n",
       " '>:p': 1.0,\n",
       " '>:[': -2.1,\n",
       " '>:\\\\': -1.7,\n",
       " '>;(': -2.9,\n",
       " '>;)': 0.1,\n",
       " '>_>^': 2.1,\n",
       " '@:': -2.1,\n",
       " '@>-->--': 2.1,\n",
       " \"@}-;-'---\": 2.2,\n",
       " 'aas': 2.5,\n",
       " 'aayf': 2.7,\n",
       " 'afu': -2.9,\n",
       " 'alol': 2.8,\n",
       " 'ambw': 2.9,\n",
       " 'aml': 3.4,\n",
       " 'atab': -1.9,\n",
       " 'awol': -1.3,\n",
       " 'ayc': 0.2,\n",
       " 'ayor': -1.2,\n",
       " 'aug-00': 0.3,\n",
       " 'bfd': -2.7,\n",
       " 'bfe': -2.6,\n",
       " 'bff': 2.9,\n",
       " 'bffn': 1.0,\n",
       " 'bl': 2.3,\n",
       " 'bsod': -2.2,\n",
       " 'btd': -2.1,\n",
       " 'btdt': -0.1,\n",
       " 'bz': 0.4,\n",
       " 'b^d': 2.6,\n",
       " 'cwot': -2.3,\n",
       " \"d-':\": -2.5,\n",
       " 'd8': -3.2,\n",
       " 'd:': 1.2,\n",
       " 'd:<': -3.2,\n",
       " 'd;': -2.9,\n",
       " 'd=': 1.5,\n",
       " 'doa': -2.3,\n",
       " 'dx': -3.0,\n",
       " 'ez': 1.5,\n",
       " 'fav': 2.0,\n",
       " 'fcol': -1.8,\n",
       " 'ff': 1.8,\n",
       " 'ffs': -2.8,\n",
       " 'fkm': -2.4,\n",
       " 'foaf': 1.8,\n",
       " 'ftw': 2.0,\n",
       " 'fu': -3.7,\n",
       " 'fubar': -3.0,\n",
       " 'fwb': 2.5,\n",
       " 'fyi': 0.8,\n",
       " 'fysa': 0.4,\n",
       " 'g1': 1.4,\n",
       " 'gg': 1.2,\n",
       " 'gga': 1.7,\n",
       " 'gigo': -0.6,\n",
       " 'gj': 2.0,\n",
       " 'gl': 1.3,\n",
       " 'gla': 2.5,\n",
       " 'gn': 1.2,\n",
       " 'gr8': 2.7,\n",
       " 'grrr': -0.4,\n",
       " 'gt': 1.1,\n",
       " 'h&k': 2.3,\n",
       " 'hagd': 2.2,\n",
       " 'hagn': 2.2,\n",
       " 'hago': 1.2,\n",
       " 'hak': 1.9,\n",
       " 'hand': 2.2,\n",
       " 'heart': 3.2,\n",
       " 'hearts': 3.3,\n",
       " 'hho1/2k': 1.4,\n",
       " 'hhoj': 2.0,\n",
       " 'hhok': 0.9,\n",
       " 'hugz': 2.0,\n",
       " 'hi5': 1.9,\n",
       " 'idk': -0.4,\n",
       " 'ijs': 0.7,\n",
       " 'ilu': 3.4,\n",
       " 'iluaaf': 2.7,\n",
       " 'ily': 3.4,\n",
       " 'ily2': 2.6,\n",
       " 'iou': 0.7,\n",
       " 'iyq': 2.3,\n",
       " 'j/j': 2.0,\n",
       " 'j/k': 1.6,\n",
       " 'j/p': 1.4,\n",
       " 'j/t': -0.2,\n",
       " 'j/w': 1.0,\n",
       " 'j4f': 1.4,\n",
       " 'j4g': 1.7,\n",
       " 'jho': 0.8,\n",
       " 'jhomf': 1.0,\n",
       " 'jj': 1.0,\n",
       " 'jk': 0.9,\n",
       " 'jp': 0.8,\n",
       " 'jt': 0.9,\n",
       " 'jw': 1.6,\n",
       " 'jealz': -1.2,\n",
       " 'k4y': 2.3,\n",
       " 'kfy': 2.3,\n",
       " 'kia': -3.2,\n",
       " 'kk': 1.5,\n",
       " 'kmuf': 2.2,\n",
       " 'l': 2.0,\n",
       " 'l&r': 2.2,\n",
       " 'laoj': 1.3,\n",
       " 'lmao': 2.9,\n",
       " 'lmbao': 1.8,\n",
       " 'lmfao': 2.5,\n",
       " 'lmso': 2.7,\n",
       " 'lol': 1.8,\n",
       " 'lolz': 2.7,\n",
       " 'lts': 1.6,\n",
       " 'ly': 2.6,\n",
       " 'ly4e': 2.7,\n",
       " 'lya': 3.3,\n",
       " 'lyb': 3.0,\n",
       " 'lyl': 3.1,\n",
       " 'lylab': 2.7,\n",
       " 'lylas': 2.6,\n",
       " 'lylb': 1.6,\n",
       " 'm8': 1.4,\n",
       " 'mia': -1.2,\n",
       " 'mml': 2.0,\n",
       " 'mofo': -2.4,\n",
       " 'muah': 2.3,\n",
       " 'mubar': -1.0,\n",
       " 'musm': 0.9,\n",
       " 'mwah': 2.5,\n",
       " 'n1': 1.9,\n",
       " 'nbd': 1.3,\n",
       " 'nbif': -0.5,\n",
       " 'nfc': -2.7,\n",
       " 'nfw': -2.4,\n",
       " 'nh': 2.2,\n",
       " 'nimby': -0.8,\n",
       " 'nimjd': -0.7,\n",
       " 'nimq': -0.2,\n",
       " 'nimy': -1.4,\n",
       " 'nitl': -1.5,\n",
       " 'nme': -2.1,\n",
       " 'noyb': -0.7,\n",
       " 'np': 1.4,\n",
       " 'ntmu': 1.4,\n",
       " 'o-8': -0.5,\n",
       " 'o-:': -0.3,\n",
       " 'o-|': -1.1,\n",
       " 'o.o': -0.8,\n",
       " 'O.o': -0.6,\n",
       " 'o.O': -0.6,\n",
       " 'o:': -0.2,\n",
       " 'o:)': 1.5,\n",
       " 'o:-)': 2.0,\n",
       " 'o:-3': 2.2,\n",
       " 'o:3': 2.3,\n",
       " 'o:<': -0.3,\n",
       " 'o;^)': 1.6,\n",
       " 'ok': 1.2,\n",
       " 'o_o': -0.5,\n",
       " 'O_o': -0.5,\n",
       " 'o_O': -0.5,\n",
       " 'pita': -2.4,\n",
       " 'pls': 0.3,\n",
       " 'plz': 0.3,\n",
       " 'pmbi': 0.8,\n",
       " 'pmfji': 0.3,\n",
       " 'pmji': 0.7,\n",
       " 'po': -2.6,\n",
       " 'ptl': 2.6,\n",
       " 'pu': -1.1,\n",
       " 'qq': -2.2,\n",
       " 'qt': 1.8,\n",
       " 'r&r': 2.4,\n",
       " 'rofl': 2.7,\n",
       " 'roflmao': 2.5,\n",
       " 'rotfl': 2.6,\n",
       " 'rotflmao': 2.8,\n",
       " 'rotflmfao': 2.5,\n",
       " 'rotflol': 3.0,\n",
       " 'rotgl': 2.9,\n",
       " 'rotglmao': 1.8,\n",
       " 's:': -1.1,\n",
       " 'sapfu': -1.1,\n",
       " 'sete': 2.8,\n",
       " 'sfete': 2.7,\n",
       " 'sgtm': 2.4,\n",
       " 'slap': 0.6,\n",
       " 'slaw': 2.1,\n",
       " 'smh': -1.3,\n",
       " 'snafu': -2.5,\n",
       " 'sob': -1.0,\n",
       " 'swak': 2.3,\n",
       " 'tgif': 2.3,\n",
       " 'thks': 1.4,\n",
       " 'thx': 1.5,\n",
       " 'tia': 2.3,\n",
       " 'tmi': -0.3,\n",
       " 'tnx': 1.1,\n",
       " 'true': 1.8,\n",
       " 'tx': 1.5,\n",
       " 'txs': 1.1,\n",
       " 'ty': 1.6,\n",
       " 'tyvm': 2.5,\n",
       " 'urw': 1.9,\n",
       " 'vbg': 2.1,\n",
       " 'vbs': 3.1,\n",
       " 'vip': 2.3,\n",
       " 'vwd': 2.6,\n",
       " 'vwp': 2.1,\n",
       " 'wag': -0.2,\n",
       " 'wd': 2.7,\n",
       " 'wilco': 0.9,\n",
       " 'wp': 1.0,\n",
       " 'wtf': -2.8,\n",
       " 'wtg': 2.1,\n",
       " 'wth': -2.4,\n",
       " 'x-d': 2.6,\n",
       " 'x-p': 1.7,\n",
       " 'xd': 2.8,\n",
       " 'xlnt': 3.0,\n",
       " 'xoxo': 3.0,\n",
       " 'xoxozzz': 2.3,\n",
       " 'xp': 1.6,\n",
       " 'xqzt': 1.6,\n",
       " 'xtc': 0.8,\n",
       " 'yolo': 1.1,\n",
       " 'yoyo': 0.4,\n",
       " 'yvw': 1.6,\n",
       " 'yw': 1.8,\n",
       " 'ywia': 2.5,\n",
       " 'zzz': -1.2,\n",
       " '[-;': 0.5,\n",
       " '[:': 1.3,\n",
       " '[;': 1.0,\n",
       " '[=': 1.7,\n",
       " '\\\\-:': -1.0,\n",
       " '\\\\:': -1.0,\n",
       " '\\\\:<': -1.7,\n",
       " '\\\\=': -1.1,\n",
       " '\\\\^:': -1.3,\n",
       " '\\\\o/': 2.2,\n",
       " '\\\\o:': -1.2,\n",
       " ']-:': -2.1,\n",
       " ']:': -1.6,\n",
       " ']:<': -2.5,\n",
       " '^<_<': 1.4,\n",
       " '^urs': -2.8,\n",
       " 'abandon': -1.9,\n",
       " 'abandoned': -2.0,\n",
       " 'abandoner': -1.9,\n",
       " 'abandoners': -1.9,\n",
       " 'abandoning': -1.6,\n",
       " 'abandonment': -2.4,\n",
       " 'abandonments': -1.7,\n",
       " 'abandons': -1.3,\n",
       " 'abducted': -2.3,\n",
       " 'abduction': -2.8,\n",
       " 'abductions': -2.0,\n",
       " 'abhor': -2.0,\n",
       " 'abhorred': -2.4,\n",
       " 'abhorrent': -3.1,\n",
       " 'abhors': -2.9,\n",
       " 'abilities': 1.0,\n",
       " 'ability': 1.3,\n",
       " 'aboard': 0.1,\n",
       " 'absentee': -1.1,\n",
       " 'absentees': -0.8,\n",
       " 'absolve': 1.2,\n",
       " 'absolved': 1.5,\n",
       " 'absolves': 1.3,\n",
       " 'absolving': 1.6,\n",
       " 'abuse': -3.2,\n",
       " 'abused': -2.3,\n",
       " 'abuser': -2.6,\n",
       " 'abusers': -2.6,\n",
       " 'abuses': -2.6,\n",
       " 'abusing': -2.0,\n",
       " 'abusive': -3.2,\n",
       " 'abusively': -2.8,\n",
       " 'abusiveness': -2.5,\n",
       " 'abusivenesses': -3.0,\n",
       " 'accept': 1.6,\n",
       " 'acceptabilities': 1.6,\n",
       " 'acceptability': 1.1,\n",
       " 'acceptable': 1.3,\n",
       " 'acceptableness': 1.3,\n",
       " 'acceptably': 1.5,\n",
       " 'acceptance': 2.0,\n",
       " 'acceptances': 1.7,\n",
       " 'acceptant': 1.6,\n",
       " 'acceptation': 1.3,\n",
       " 'acceptations': 0.9,\n",
       " 'accepted': 1.1,\n",
       " 'accepting': 1.6,\n",
       " 'accepts': 1.3,\n",
       " 'accident': -2.1,\n",
       " 'accidental': -0.3,\n",
       " 'accidentally': -1.4,\n",
       " 'accidents': -1.3,\n",
       " 'accomplish': 1.8,\n",
       " 'accomplished': 1.9,\n",
       " 'accomplishes': 1.7,\n",
       " 'accusation': -1.0,\n",
       " 'accusations': -1.3,\n",
       " 'accuse': -0.8,\n",
       " 'accused': -1.2,\n",
       " 'accuses': -1.4,\n",
       " 'accusing': -0.7,\n",
       " 'ache': -1.6,\n",
       " 'ached': -1.6,\n",
       " 'aches': -1.0,\n",
       " 'achievable': 1.3,\n",
       " 'aching': -2.2,\n",
       " 'acquit': 0.8,\n",
       " 'acquits': 0.1,\n",
       " 'acquitted': 1.0,\n",
       " 'acquitting': 1.3,\n",
       " 'acrimonious': -1.7,\n",
       " 'active': 1.7,\n",
       " 'actively': 1.3,\n",
       " 'activeness': 0.6,\n",
       " 'activenesses': 0.8,\n",
       " 'actives': 1.1,\n",
       " 'adequate': 0.9,\n",
       " 'admirability': 2.4,\n",
       " 'admirable': 2.6,\n",
       " 'admirableness': 2.2,\n",
       " 'admirably': 2.5,\n",
       " 'admiral': 1.3,\n",
       " 'admirals': 1.5,\n",
       " 'admiralties': 1.6,\n",
       " 'admiralty': 1.2,\n",
       " 'admiration': 2.5,\n",
       " 'admirations': 1.6,\n",
       " 'admire': 2.1,\n",
       " 'admired': 2.3,\n",
       " 'admirer': 1.8,\n",
       " 'admirers': 1.7,\n",
       " 'admires': 1.5,\n",
       " 'admiring': 1.6,\n",
       " 'admiringly': 2.3,\n",
       " 'admit': 0.8,\n",
       " 'admits': 1.2,\n",
       " 'admitted': 0.4,\n",
       " 'admonished': -1.9,\n",
       " 'adopt': 0.7,\n",
       " 'adopts': 0.7,\n",
       " 'adorability': 2.2,\n",
       " 'adorable': 2.2,\n",
       " 'adorableness': 2.5,\n",
       " 'adorably': 2.1,\n",
       " 'adoration': 2.9,\n",
       " 'adorations': 2.2,\n",
       " 'adore': 2.6,\n",
       " 'adored': 1.8,\n",
       " 'adorer': 1.7,\n",
       " 'adorers': 2.1,\n",
       " 'adores': 1.6,\n",
       " 'adoring': 2.6,\n",
       " 'adoringly': 2.4,\n",
       " 'adorn': 0.9,\n",
       " 'adorned': 0.8,\n",
       " 'adorner': 1.3,\n",
       " 'adorners': 0.9,\n",
       " 'adorning': 1.0,\n",
       " 'adornment': 1.3,\n",
       " 'adornments': 0.8,\n",
       " 'adorns': 0.5,\n",
       " 'advanced': 1.0,\n",
       " 'advantage': 1.0,\n",
       " 'advantaged': 1.4,\n",
       " 'advantageous': 1.5,\n",
       " 'advantageously': 1.9,\n",
       " 'advantageousness': 1.6,\n",
       " 'advantages': 1.5,\n",
       " 'advantaging': 1.6,\n",
       " 'adventure': 1.3,\n",
       " 'adventured': 1.3,\n",
       " 'adventurer': 1.2,\n",
       " 'adventurers': 0.9,\n",
       " 'adventures': 1.4,\n",
       " 'adventuresome': 1.7,\n",
       " 'adventuresomeness': 1.3,\n",
       " 'adventuress': 0.8,\n",
       " 'adventuresses': 1.4,\n",
       " 'adventuring': 2.3,\n",
       " 'adventurism': 1.5,\n",
       " 'adventurist': 1.4,\n",
       " 'adventuristic': 1.7,\n",
       " 'adventurists': 1.2,\n",
       " 'adventurous': 1.4,\n",
       " 'adventurously': 1.3,\n",
       " 'adventurousness': 1.8,\n",
       " 'adversarial': -1.5,\n",
       " 'adversaries': -1.0,\n",
       " 'adversary': -0.8,\n",
       " 'adversative': -1.2,\n",
       " 'adversatively': -0.1,\n",
       " 'adversatives': -1.0,\n",
       " 'adverse': -1.5,\n",
       " 'adversely': -0.8,\n",
       " 'adverseness': -0.6,\n",
       " 'adversities': -1.5,\n",
       " 'adversity': -1.8,\n",
       " 'affected': -0.6,\n",
       " 'affection': 2.4,\n",
       " 'affectional': 1.9,\n",
       " 'affectionally': 1.5,\n",
       " 'affectionate': 1.9,\n",
       " 'affectionately': 2.2,\n",
       " 'affectioned': 1.8,\n",
       " 'affectionless': -2.0,\n",
       " 'affections': 1.5,\n",
       " 'afflicted': -1.5,\n",
       " 'affronted': 0.2,\n",
       " 'aggravate': -2.5,\n",
       " 'aggravated': -1.9,\n",
       " 'aggravates': -1.9,\n",
       " 'aggravating': -1.2,\n",
       " 'aggress': -1.3,\n",
       " 'aggressed': -1.4,\n",
       " 'aggresses': -0.5,\n",
       " 'aggressing': -0.6,\n",
       " 'aggression': -1.2,\n",
       " 'aggressions': -1.3,\n",
       " 'aggressive': -0.6,\n",
       " 'aggressively': -1.3,\n",
       " 'aggressiveness': -1.8,\n",
       " 'aggressivities': -1.4,\n",
       " 'aggressivity': -0.6,\n",
       " 'aggressor': -0.8,\n",
       " 'aggressors': -0.9,\n",
       " 'aghast': -1.9,\n",
       " 'agitate': -1.7,\n",
       " 'agitated': -2.0,\n",
       " 'agitatedly': -1.6,\n",
       " 'agitates': -1.4,\n",
       " 'agitating': -1.8,\n",
       " 'agitation': -1.0,\n",
       " 'agitational': -1.2,\n",
       " 'agitations': -1.3,\n",
       " 'agitative': -1.3,\n",
       " 'agitato': -0.1,\n",
       " 'agitator': -1.4,\n",
       " 'agitators': -2.1,\n",
       " 'agog': 1.9,\n",
       " 'agonise': -2.1,\n",
       " 'agonised': -2.3,\n",
       " 'agonises': -2.4,\n",
       " 'agonising': -1.5,\n",
       " 'agonize': -2.3,\n",
       " 'agonized': -2.2,\n",
       " 'agonizes': -2.3,\n",
       " 'agonizing': -2.7,\n",
       " 'agonizingly': -2.3,\n",
       " 'agony': -1.8,\n",
       " 'agree': 1.5,\n",
       " 'agreeability': 1.9,\n",
       " 'agreeable': 1.8,\n",
       " 'agreeableness': 1.8,\n",
       " 'agreeablenesses': 1.3,\n",
       " 'agreeably': 1.6,\n",
       " 'agreed': 1.1,\n",
       " 'agreeing': 1.4,\n",
       " 'agreement': 2.2,\n",
       " 'agreements': 1.1,\n",
       " 'agrees': 0.8,\n",
       " 'alarm': -1.4,\n",
       " 'alarmed': -1.4,\n",
       " 'alarming': -0.5,\n",
       " 'alarmingly': -2.6,\n",
       " 'alarmism': -0.3,\n",
       " 'alarmists': -1.1,\n",
       " 'alarms': -1.1,\n",
       " 'alas': -1.1,\n",
       " 'alert': 1.2,\n",
       " 'alienation': -1.1,\n",
       " 'alive': 1.6,\n",
       " 'allergic': -1.2,\n",
       " 'allow': 0.9,\n",
       " 'alone': -1.0,\n",
       " 'alright': 1.0,\n",
       " 'amaze': 2.5,\n",
       " 'amazed': 2.2,\n",
       " 'amazedly': 2.1,\n",
       " 'amazement': 2.5,\n",
       " 'amazements': 2.2,\n",
       " 'amazes': 2.2,\n",
       " 'amazing': 2.8,\n",
       " 'amazon': 0.7,\n",
       " 'amazonite': 0.2,\n",
       " 'amazons': -0.1,\n",
       " 'amazonstone': 1.0,\n",
       " 'amazonstones': 0.2,\n",
       " 'ambitious': 2.1,\n",
       " 'ambivalent': 0.5,\n",
       " 'amor': 3.0,\n",
       " 'amoral': -1.6,\n",
       " 'amoralism': -0.7,\n",
       " 'amoralisms': -0.7,\n",
       " 'amoralities': -1.2,\n",
       " 'amorality': -1.5,\n",
       " 'amorally': -1.0,\n",
       " 'amoretti': 0.2,\n",
       " 'amoretto': 0.6,\n",
       " 'amorettos': 0.3,\n",
       " 'amorino': 1.2,\n",
       " 'amorist': 1.6,\n",
       " 'amoristic': 1.0,\n",
       " 'amorists': 0.1,\n",
       " 'amoroso': 2.3,\n",
       " 'amorous': 1.8,\n",
       " 'amorously': 2.3,\n",
       " 'amorousness': 2.0,\n",
       " 'amorphous': -0.2,\n",
       " 'amorphously': 0.1,\n",
       " 'amorphousness': 0.3,\n",
       " 'amort': -2.1,\n",
       " 'amortise': 0.5,\n",
       " 'amortised': -0.2,\n",
       " 'amortises': 0.1,\n",
       " 'amortizable': 0.5,\n",
       " 'amortization': 0.6,\n",
       " 'amortizations': 0.2,\n",
       " 'amortize': -0.1,\n",
       " 'amortized': 0.8,\n",
       " 'amortizes': 0.6,\n",
       " 'amortizing': 0.8,\n",
       " 'amusable': 0.7,\n",
       " 'amuse': 1.7,\n",
       " 'amused': 1.8,\n",
       " 'amusedly': 2.2,\n",
       " 'amusement': 1.5,\n",
       " 'amusements': 1.5,\n",
       " 'amuser': 1.1,\n",
       " 'amusers': 1.3,\n",
       " 'amuses': 1.7,\n",
       " 'amusia': 0.3,\n",
       " 'amusias': -0.4,\n",
       " 'amusing': 1.6,\n",
       " 'amusingly': 0.8,\n",
       " 'amusingness': 1.8,\n",
       " 'amusive': 1.7,\n",
       " 'anger': -2.7,\n",
       " 'angered': -2.3,\n",
       " 'angering': -2.2,\n",
       " 'angerly': -1.9,\n",
       " 'angers': -2.3,\n",
       " 'angrier': -2.3,\n",
       " 'angriest': -3.1,\n",
       " 'angrily': -1.8,\n",
       " 'angriness': -1.7,\n",
       " 'angry': -2.3,\n",
       " 'anguish': -2.9,\n",
       " 'anguished': -1.8,\n",
       " 'anguishes': -2.1,\n",
       " 'anguishing': -2.7,\n",
       " 'animosity': -1.9,\n",
       " 'annoy': -1.9,\n",
       " 'annoyance': -1.3,\n",
       " 'annoyances': -1.8,\n",
       " 'annoyed': -1.6,\n",
       " 'annoyer': -2.2,\n",
       " 'annoyers': -1.5,\n",
       " 'annoying': -1.7,\n",
       " 'annoys': -1.8,\n",
       " 'antagonism': -1.9,\n",
       " 'antagonisms': -1.2,\n",
       " 'antagonist': -1.9,\n",
       " 'antagonistic': -1.7,\n",
       " 'antagonistically': -2.2,\n",
       " 'antagonists': -1.7,\n",
       " 'antagonize': -2.0,\n",
       " 'antagonized': -1.4,\n",
       " 'antagonizes': -0.5,\n",
       " 'antagonizing': -2.7,\n",
       " 'anti': -1.3,\n",
       " 'anticipation': 0.4,\n",
       " 'anxieties': -0.6,\n",
       " 'anxiety': -0.7,\n",
       " 'anxious': -1.0,\n",
       " 'anxiously': -0.9,\n",
       " 'anxiousness': -1.0,\n",
       " 'aok': 2.0,\n",
       " 'apathetic': -1.2,\n",
       " 'apathetically': -0.4,\n",
       " 'apathies': -0.6,\n",
       " 'apathy': -1.2,\n",
       " 'apeshit': -0.9,\n",
       " 'apocalyptic': -3.4,\n",
       " 'apologise': 1.6,\n",
       " 'apologised': 0.4,\n",
       " 'apologises': 0.8,\n",
       " 'apologising': 0.2,\n",
       " 'apologize': 0.4,\n",
       " 'apologized': 1.3,\n",
       " 'apologizes': 1.5,\n",
       " 'apologizing': -0.3,\n",
       " 'apology': 0.2,\n",
       " 'appall': -2.4,\n",
       " 'appalled': -2.0,\n",
       " 'appalling': -1.5,\n",
       " 'appallingly': -2.0,\n",
       " 'appalls': -1.9,\n",
       " 'appease': 1.1,\n",
       " 'appeased': 0.9,\n",
       " 'appeases': 0.9,\n",
       " 'appeasing': 1.0,\n",
       " 'applaud': 2.0,\n",
       " 'applauded': 1.5,\n",
       " 'applauding': 2.1,\n",
       " 'applauds': 1.4,\n",
       " 'applause': 1.8,\n",
       " 'appreciate': 1.7,\n",
       " 'appreciated': 2.3,\n",
       " 'appreciates': 2.3,\n",
       " 'appreciating': 1.9,\n",
       " 'appreciation': 2.3,\n",
       " 'appreciations': 1.7,\n",
       " 'appreciative': 2.6,\n",
       " 'appreciatively': 1.8,\n",
       " 'appreciativeness': 1.6,\n",
       " 'appreciator': 2.6,\n",
       " 'appreciators': 1.5,\n",
       " 'appreciatory': 1.7,\n",
       " 'apprehensible': 1.1,\n",
       " 'apprehensibly': -0.2,\n",
       " 'apprehension': -2.1,\n",
       " 'apprehensions': -0.9,\n",
       " 'apprehensively': -0.3,\n",
       " 'apprehensiveness': -0.7,\n",
       " 'approval': 2.1,\n",
       " 'approved': 1.8,\n",
       " 'approves': 1.7,\n",
       " 'ardent': 2.1,\n",
       " 'arguable': -1.0,\n",
       " 'arguably': -1.0,\n",
       " 'argue': -1.4,\n",
       " 'argued': -1.5,\n",
       " 'arguer': -1.6,\n",
       " 'arguers': -1.4,\n",
       " 'argues': -1.6,\n",
       " 'arguing': -2.0,\n",
       " 'argument': -1.5,\n",
       " 'argumentative': -1.5,\n",
       " 'argumentatively': -1.8,\n",
       " 'argumentive': -1.5,\n",
       " 'arguments': -1.7,\n",
       " 'arrest': -1.4,\n",
       " 'arrested': -2.1,\n",
       " 'arrests': -1.9,\n",
       " 'arrogance': -2.4,\n",
       " 'arrogances': -1.9,\n",
       " 'arrogant': -2.2,\n",
       " 'arrogantly': -1.8,\n",
       " 'ashamed': -2.1,\n",
       " 'ashamedly': -1.7,\n",
       " 'ass': -2.5,\n",
       " 'assassination': -2.9,\n",
       " 'assassinations': -2.7,\n",
       " 'assault': -2.8,\n",
       " 'assaulted': -2.4,\n",
       " 'assaulting': -2.3,\n",
       " 'assaultive': -2.8,\n",
       " 'assaults': -2.5,\n",
       " 'asset': 1.5,\n",
       " 'assets': 0.7,\n",
       " 'assfucking': -2.5,\n",
       " 'assholes': -2.8,\n",
       " 'assurance': 1.4,\n",
       " 'assurances': 1.4,\n",
       " 'assure': 1.4,\n",
       " 'assured': 1.5,\n",
       " 'assuredly': 1.6,\n",
       " 'assuredness': 1.4,\n",
       " 'assurer': 0.9,\n",
       " 'assurers': 1.1,\n",
       " 'assures': 1.3,\n",
       " 'assurgent': 1.3,\n",
       " 'assuring': 1.6,\n",
       " 'assuror': 0.5,\n",
       " 'assurors': 0.7,\n",
       " 'astonished': 1.6,\n",
       " 'astound': 1.7,\n",
       " 'astounded': 1.8,\n",
       " 'astounding': 1.8,\n",
       " 'astoundingly': 2.1,\n",
       " 'astounds': 2.1,\n",
       " 'attachment': 1.2,\n",
       " 'attachments': 1.1,\n",
       " 'attack': -2.1,\n",
       " 'attacked': -2.0,\n",
       " 'attacker': -2.7,\n",
       " 'attackers': -2.7,\n",
       " 'attacking': -2.0,\n",
       " 'attacks': -1.9,\n",
       " 'attract': 1.5,\n",
       " 'attractancy': 0.9,\n",
       " 'attractant': 1.3,\n",
       " 'attractants': 1.4,\n",
       " 'attracted': 1.8,\n",
       " 'attracting': 2.1,\n",
       " 'attraction': 2.0,\n",
       " 'attractions': 1.8,\n",
       " 'attractive': 1.9,\n",
       " 'attractively': 2.2,\n",
       " 'attractiveness': 1.8,\n",
       " 'attractivenesses': 2.1,\n",
       " 'attractor': 1.2,\n",
       " 'attractors': 1.2,\n",
       " 'attracts': 1.7,\n",
       " 'audacious': 0.9,\n",
       " 'authority': 0.3,\n",
       " 'aversion': -1.9,\n",
       " 'aversions': -1.1,\n",
       " 'aversive': -1.6,\n",
       " 'aversively': -0.8,\n",
       " 'avert': -0.7,\n",
       " 'averted': -0.3,\n",
       " 'averts': -0.4,\n",
       " 'avid': 1.2,\n",
       " 'avoid': -1.2,\n",
       " 'avoidance': -1.7,\n",
       " 'avoidances': -1.1,\n",
       " 'avoided': -1.4,\n",
       " 'avoider': -1.8,\n",
       " 'avoiders': -1.4,\n",
       " 'avoiding': -1.4,\n",
       " 'avoids': -0.7,\n",
       " 'await': 0.4,\n",
       " 'awaited': -0.1,\n",
       " 'awaits': 0.3,\n",
       " 'award': 2.5,\n",
       " 'awardable': 2.4,\n",
       " 'awarded': 1.7,\n",
       " 'awardee': 1.8,\n",
       " 'awardees': 1.2,\n",
       " 'awarder': 0.9,\n",
       " 'awarders': 1.3,\n",
       " 'awarding': 1.9,\n",
       " 'awards': 2.0,\n",
       " 'awesome': 3.1,\n",
       " 'awful': -2.0,\n",
       " 'awkward': -0.6,\n",
       " 'awkwardly': -1.3,\n",
       " 'awkwardness': -0.7,\n",
       " 'axe': -0.4,\n",
       " 'axed': -1.3,\n",
       " 'backed': 0.1,\n",
       " 'backing': 0.1,\n",
       " 'backs': -0.2,\n",
       " 'bad': -2.5,\n",
       " 'badass': 1.4,\n",
       " 'badly': -2.1,\n",
       " 'bailout': -0.4,\n",
       " 'bamboozle': -1.5,\n",
       " 'bamboozled': -1.5,\n",
       " 'bamboozles': -1.5,\n",
       " 'ban': -2.6,\n",
       " 'banish': -1.9,\n",
       " 'bankrupt': -2.6,\n",
       " 'bankster': -2.1,\n",
       " 'banned': -2.0,\n",
       " 'bargain': 0.8,\n",
       " 'barrier': -0.5,\n",
       " 'bashful': -0.1,\n",
       " 'bashfully': 0.2,\n",
       " 'bashfulness': -0.8,\n",
       " 'bastard': -2.5,\n",
       " 'bastardies': -1.8,\n",
       " 'bastardise': -2.1,\n",
       " 'bastardised': -2.3,\n",
       " 'bastardises': -2.3,\n",
       " 'bastardising': -2.6,\n",
       " 'bastardization': -2.4,\n",
       " 'bastardizations': -2.1,\n",
       " 'bastardize': -2.4,\n",
       " 'bastardized': -2.0,\n",
       " 'bastardizes': -1.8,\n",
       " 'bastardizing': -2.3,\n",
       " 'bastardly': -2.7,\n",
       " 'bastards': -3.0,\n",
       " 'bastardy': -2.7,\n",
       " 'battle': -1.6,\n",
       " 'battled': -1.2,\n",
       " 'battlefield': -1.6,\n",
       " 'battlefields': -0.9,\n",
       " 'battlefront': -1.2,\n",
       " 'battlefronts': -0.8,\n",
       " 'battleground': -1.7,\n",
       " 'battlegrounds': -0.6,\n",
       " 'battlement': -0.4,\n",
       " 'battlements': -0.4,\n",
       " 'battler': -0.8,\n",
       " 'battlers': -0.2,\n",
       " 'battles': -1.6,\n",
       " 'battleship': -0.1,\n",
       " 'battleships': -0.5,\n",
       " 'battlewagon': -0.3,\n",
       " 'battlewagons': -0.5,\n",
       " 'battling': -1.1,\n",
       " 'beaten': -1.8,\n",
       " 'beatific': 1.8,\n",
       " 'beating': -2.0,\n",
       " 'beaut': 1.6,\n",
       " 'beauteous': 2.5,\n",
       " 'beauteously': 2.6,\n",
       " ...}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa.lexicon #SentimentIntensityAnalyzer.lexicon contains that dictionary of tokens and their scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28f4db15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"( '}{' )\", 1.6),\n",
       " (\"can't stand\", -2.0),\n",
       " ('fed up', -1.8),\n",
       " ('screwed up', -1.5)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(tok, score) for tok, score in sa.lexicon.items() if \" \" in tok]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1269b504",
   "metadata": {},
   "source": [
    "Out of 7500 tokens defined in VADER, only 3 contain spaces, and only 2 of those are actually n-grams; the other is an emoticon for “kiss.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fbfe3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.661, 'pos': 0.339, 'compound': 0.6249}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa.polarity_scores(text=\"Python is very readable and it's great for NLP.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdd6a4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.737, 'pos': 0.263, 'compound': 0.431}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa.polarity_scores(text= \"Python is not a bad choice for most applications.\") #The VADER algorithm considers the intensity of sentiment polarity in three separate scores (positive, negative, and neutral) and then combines them together into a compound positivity sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04a09e4",
   "metadata": {},
   "source": [
    "Let’s see how well this rule-based approach does for the example statements we mentioned earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32fd0e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"Absolutely perfect! Love it! :-) :-) :-)\", \n",
    "          \"Horrible! Completely useless. :(\", \n",
    "          \"It was OK. Some good and some bad things.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7e9d106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.1531: It was OK. Some good and some bad things.\n"
     ]
    }
   ],
   "source": [
    "for doc in corpus:\n",
    "    scores = sa.polarity_scores(doc)\n",
    "print('{:+}: {}'.format(scores['compound'], doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4864e129",
   "metadata": {},
   "source": [
    "Note books result\n",
    "+0.9428: Absolutely perfect! Love it! :-) :-) :-)\n",
    "-0.8768: Horrible! Completely useless. :(\n",
    "+0.3254: It was OK. Some good and some bad things."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d37fe12",
   "metadata": {},
   "source": [
    "So the only drawback is that VADER doesn’t look at all the words in a document, only about 7,500. what if you don’t want to have to code your own understanding of the words in a dictionary of thousands of words or add a bunch of custom words to the dictionary in SentimentIntensityAnalyzer.lexicon? The rule-based approach might be impossible if you don’t understand the language, because you wouldn’t know what scores to put in the dictionary (lexicon)!\n",
    "###### That’s what machine learning sentiment analyzers are for.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b0e2ccb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nlpia\n",
      "  Downloading nlpia-0.5.2-py2.py3-none-any.whl (32.0 MB)\n",
      "Requirement already satisfied: future in c:\\users\\emz\\anaconda3\\lib\\site-packages (from nlpia) (0.18.2)\n",
      "Collecting python-Levenshtein\n",
      "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\emz\\anaconda3\\lib\\site-packages (from nlpia) (4.64.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\emz\\anaconda3\\lib\\site-packages (from nlpia) (0.11.2)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.4.1-cp39-cp39-win_amd64.whl (11.8 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 519, in read\n",
      "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 62, in read\n",
      "    data = self.__fp.read(amt)\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\http\\client.py\", line 463, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\http\\client.py\", line 507, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 173, in _main\n",
      "    status = self.run(options, args)\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 203, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 315, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 94, in resolve\n",
      "    result = self._result = resolver.resolve(\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 472, in resolve\n",
      "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 366, in resolve\n",
      "    failure_causes = self._attempt_to_pin_criterion(name)\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 212, in _attempt_to_pin_criterion\n",
      "    criteria = self._get_updated_criteria(candidate)\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 203, in _get_updated_criteria\n",
      "    self._add_to_criteria(criteria, requirement, parent=candidate)\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 172, in _add_to_criteria\n",
      "    if not criterion.candidates:\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\structs.py\", line 151, in __bool__\n",
      "    return bool(self._sequence)\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 140, in __bool__\n",
      "    return any(self)\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 128, in <genexpr>\n",
      "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 32, in _iter_built\n",
      "    candidate = func()\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 204, in _make_candidate_from_link\n",
      "    self._link_candidate_cache[link] = LinkCandidate(\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 295, in __init__\n",
      "    super().__init__(\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 156, in __init__\n",
      "    self.dist = self._prepare()\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 227, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 305, in _prepare_distribution\n",
      "    return self._factory.preparer.prepare_linked_requirement(\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 508, in prepare_linked_requirement\n",
      "    return self._prepare_linked_requirement(req, parallel_builds)\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 550, in _prepare_linked_requirement\n",
      "    local_file = unpack_url(\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 239, in unpack_url\n",
      "    file = get_http_url(\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 102, in get_http_url\n",
      "    from_path, content_type = download(link, temp_dir.path)\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\site-packages\\pip\\_internal\\network\\download.py\", line 145, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\progress_bars.py\", line 144, in iter\n",
      "    for x in it:\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 63, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 576, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 541, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\contextlib.py\", line 137, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"C:\\Users\\EMZ\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 443, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\n"
     ]
    }
   ],
   "source": [
    "#!pip install nlpia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cd761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "to be contiue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
